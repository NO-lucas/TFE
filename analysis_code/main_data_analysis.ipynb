{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5323b41",
   "metadata": {},
   "source": [
    "# Lucas Henneaux : master thesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb2dff0",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efa1c65-1e8c-4345-8f66-47eab56338f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "import large_image\n",
    "\n",
    "# !pip install openslide-python tifffile\n",
    "import torch\n",
    "from timm import create_model\n",
    "from sklearn.cluster import DBSCAN\n",
    "import timm\n",
    "import random\n",
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c7d0c6-2749-4aaa-9930-6f669cf41858",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU available : ({torch.cuda.get_device_name(0)})\")\n",
    "else:\n",
    "    print(\"No GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bdc4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict(data, filename):\n",
    "    \"\"\"Save dict in json file\"\"\"\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "    print(f\"{filename} saved\")\n",
    "\n",
    "def load_dict(filename):\n",
    "    \"\"\"Load dict from json file\"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "label_path = \"labels_low_res/\"\n",
    "classes = [\"1\",\"2\",\"3\",\"41\",\"42\",\"51\",\"52\",\"54\",\"57\"]\n",
    "res=2\n",
    "if label_path == \"labels_low_res/\":\n",
    "    res = 4\n",
    "    classes = [\"1\",\"2\",\"3\",\"41\",\"5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb14c7f-129f-4fe2-b62a-27b8768bae25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import openslide\n",
    "import large_image\n",
    "import cv2\n",
    "def convert_ndpa_to_ndpi(x_ndpa, y_ndpa, slide):\n",
    "    #Converts a point in .ndpa coords system to the .ndpi one\n",
    "    \n",
    "    # useful informations (I think only available with openSlide, but not sure)\n",
    "    physical_width = float(slide.properties.get(\"hamamatsu.PhysicalWidth\", 1))\n",
    "    physical_height = float(slide.properties.get(\"hamamatsu.PhysicalHeight\", 1))\n",
    "    x_offset = float(slide.properties.get(\"hamamatsu.XOffsetFromSlideCentre\", 0))\n",
    "    y_offset = float(slide.properties.get(\"hamamatsu.YOffsetFromSlideCentre\", 0))\n",
    "\n",
    "    mpp_x = float(slide.properties[\"openslide.mpp-x\"])\n",
    "    mpp_y = float(slide.properties[\"openslide.mpp-y\"])\n",
    "\n",
    "\n",
    "    # Offsets\n",
    "    x_offset_total = x_offset\n",
    "    y_offset_total = y_offset\n",
    "\n",
    "    \n",
    "    # Conversion NDPA → NDPI\n",
    "    x_ndpi = (x_ndpa - x_offset_total) / (mpp_x * 1000) + slide.dimensions[0]/2\n",
    "    y_ndpi = (y_ndpa - y_offset_total) / (mpp_y * 1000)  + slide.dimensions[1]/2\n",
    "    \n",
    "    return int(x_ndpi), int(y_ndpi)\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import random\n",
    "\n",
    "def extract_annotations(ndpa_path, slide, low_res):\n",
    "    tree = ET.parse(ndpa_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    annotations = []\n",
    "    titles = []\n",
    "\n",
    "    for ndpviewstate in root.findall(\"ndpviewstate\"): \n",
    "        annotation = ndpviewstate.find(\"annotation\")\n",
    "\n",
    "        specialtype = annotation.find(\"specialtype\")\n",
    "        title = ndpviewstate.find(\"title\").text\n",
    "        unity = ndpviewstate.find(\"coordformat\").text\n",
    "\n",
    "        if specialtype is not None and \"rectangle\" in specialtype.text.lower():\n",
    "            points = []\n",
    "            for point in annotation.findall(\".//point\"):\n",
    "                x = float(point.find(\"x\").text)\n",
    "                y = float(point.find(\"y\").text)\n",
    "\n",
    "                px, py = convert_ndpa_to_ndpi(x, y, slide)\n",
    "                points.append((px, py))\n",
    "\n",
    "            if len(points) == 4:  # Rectangle\n",
    "                x1, y1 = min(p[0] for p in points), min(p[1] for p in points)\n",
    "                x2, y2 = max(p[0] for p in points), max(p[1] for p in points)\n",
    "\n",
    "                width = x2 - x1\n",
    "                height = y2 - y1\n",
    "                \n",
    "                min_size = 224\n",
    "\n",
    "                if low_res: #In case of low res, we have to not take too small labels into account has it would be similar to blood\n",
    "                    if width < min_size and height < min_size and title not in [\"51\",\"52\"]:\n",
    "                        continue\n",
    "\n",
    "                    else: #No more need to differentiate 51/52\n",
    "                        if title in [\"51\",\"52\"]:\n",
    "                            title = \"5\"\n",
    "\n",
    "                if width < min_size: \n",
    "                    extra_width = min_size - width\n",
    "                    shift_x = random.uniform(0, extra_width)\n",
    "                    x1 = x1 - shift_x\n",
    "                    x2 = x1 + min_size\n",
    "\n",
    "                if height < min_size:\n",
    "                    extra_height = min_size - height\n",
    "                    shift_y = random.uniform(0, extra_height)\n",
    "                    y1 = y1 - shift_y\n",
    "                    y2 = y1 + min_size\n",
    "\n",
    "                \n",
    "                annotations.append((x1, y1, x2, y2))\n",
    "                titles.append(title)\n",
    "\n",
    "    return annotations, titles\n",
    "\n",
    "\n",
    "def get_res_image(slide, res, x1, y1, x2, y2, scale_factor):\n",
    "    \"\"\" Getting image in a certain res and adapting according to this res (scale factor) \"\"\"\n",
    "   \n",
    "    return slide.read_region((int(x1), int(y1)), res, (int((x2-x1)/scale_factor), int((y2-y1)/scale_factor))).convert(\"RGB\")\n",
    "\n",
    "\n",
    "\n",
    "def process_ndpi_with_large_image(file_path, file_ndpa, output_folder_name, nth_image,classes, res=7):\n",
    "    \n",
    "    large_slide = large_image.open(file_path)\n",
    "    slide = openslide.OpenSlide(file_path)\n",
    "\n",
    "    low_res = False\n",
    "    if label_path != \"labels/\":\n",
    "        low_res = True\n",
    "    \n",
    "    annotations, titles = extract_annotations(file_ndpa, slide, low_res)\n",
    "\n",
    "    scale_factor = (slide.level_dimensions[0][0]/ slide.level_dimensions[res][0])\n",
    "\n",
    "    for i, (x1, y1, x2, y2) in enumerate(annotations):\n",
    "\n",
    "        img_region = get_res_image(slide, res, x1, y1, x2, y2, scale_factor)\n",
    "        try:\n",
    "            output_folder = os.path.join(output_folder_name, titles[i])\n",
    "        except:\n",
    "            print(f\"-----No annotation for {file_path} ------- \")\n",
    "            return []\n",
    "        if titles[i] not in classes:\n",
    "            # print(f\"----invalid label found : {titles[i]}-----\")\n",
    "            continue\n",
    "        \n",
    "        output_path = os.path.join(output_folder, f\"{titles[i]}_{i}_{nth_image}.png\")\n",
    "        \n",
    "        os.makedirs(output_folder, exist_ok=True)  # Creating folder if does not exist\n",
    "        \n",
    "        img_np = np.array(img_region)  # Convert PIL to NumPy\n",
    "        cv2.imwrite(output_path, cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "        print(f\"Saved : {output_path}\")\n",
    "    \n",
    "    return annotations\n",
    "\n",
    "\n",
    "folder_path = \"img/database\"\n",
    "\n",
    "# Folder of wi images and .ndpa files !\n",
    "files = os.listdir(folder_path)\n",
    "\n",
    "ndpi_files = [f for f in files if f.endswith(\".ndpi\")]\n",
    "ndpa_files = [f for f in files if f.endswith(\".ndpi.ndpa\")]\n",
    "\n",
    "\n",
    "for i, ndpi_file in enumerate(ndpi_files):\n",
    "    ndpa_file = ndpi_file + \".ndpa\" #Corresponding .ndpa file\n",
    "    \n",
    "    if ndpa_file in ndpa_files:\n",
    "\n",
    "        ndpi_path = os.path.join(folder_path, ndpi_file)\n",
    "        ndpa_path = os.path.join(folder_path, ndpa_file)\n",
    "        print(\"processing : \", ndpa_file,\" ...\")\n",
    "        process_ndpi_with_large_image(ndpi_path, ndpa_path, label_path, i,classes, res=res)\n",
    "    else:\n",
    "        print(f\"No .ndpa file found {ndpi_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce261f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "patch_size = (224, 224)\n",
    "min_patch_area_ratio = 0.6\n",
    "\n",
    "patches_count = {}\n",
    "\n",
    "min_patch_area = patch_size[0] * patch_size[1] * min_patch_area_ratio\n",
    "\n",
    "for class_folder in os.listdir(label_path):\n",
    "    class_folder_path = os.path.join(label_path, class_folder)\n",
    "    if os.path.isdir(class_folder_path):\n",
    "        for image_name in os.listdir(class_folder_path):\n",
    "            if image_name.lower().endswith(('png', 'jpg', 'jpeg')):\n",
    "                image_id = image_name.split('_')[2].split('.')[0]\n",
    "                image_path = os.path.join(class_folder_path, image_name)\n",
    "                \n",
    "                image = Image.open(image_path)\n",
    "                width, height = image.size\n",
    "               \n",
    "                num_patches_x = max(1, (width + patch_size[0] - 1) // patch_size[0])\n",
    "                num_patches_y = max(1, (height + patch_size[1] - 1) // patch_size[1])\n",
    "                \n",
    "                total_patches = 0\n",
    "                for x in range(num_patches_x):\n",
    "                    for y in range(num_patches_y):\n",
    "                        actual_width = min(patch_size[0], width - x * patch_size[0])\n",
    "                        actual_height = min(patch_size[1], height - y * patch_size[1])\n",
    "                        \n",
    "                        if actual_width * actual_height >= min_patch_area:\n",
    "                            total_patches += 1\n",
    "                try:\n",
    "                    patches_count[class_folder_path + \"/\" + image_name] += max(1, total_patches) \n",
    "                except:\n",
    "                    patches_count[class_folder_path + \"/\" + image_name] = max(1, total_patches)\n",
    "\n",
    "\n",
    "\n",
    "for image_name, patche_count in patches_count.items():\n",
    "    print(f\"Image {image_name} : {patche_count}\")\n",
    "\n",
    "save_dict(patches_count, \"patches_count.json\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b6216f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "patch_size = (224, 224)\n",
    "min_patch_area_ratio = 0.6\n",
    "\n",
    "patches_count = load_dict(\"patches_count.json\")\n",
    "color_histograms = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "min_patch_area = patch_size[0] * patch_size[1] * min_patch_area_ratio\n",
    "\n",
    "for class_folder in os.listdir(label_path):\n",
    "    class_folder_path = os.path.join(label_path, class_folder)\n",
    "    if os.path.isdir(class_folder_path):\n",
    "        for image_name in os.listdir(class_folder_path):\n",
    "            if image_name.lower().endswith(('png', 'jpg', 'jpeg')):\n",
    "                image_id = image_name.split('_')[2].split('.')[0]\n",
    "                image_path = os.path.join(class_folder_path, image_name)\n",
    "                \n",
    "                image = Image.open(image_path).convert(\"RGB\")\n",
    "                width, height = image.size\n",
    "                \n",
    "                num_patches_x = max(1, (width + patch_size[0] - 1) // patch_size[0])\n",
    "                num_patches_y = max(1, (height + patch_size[1] - 1) // patch_size[1])\n",
    "                \n",
    "                total_patches = 0\n",
    "                for x in range(num_patches_x):\n",
    "                    for y in range(num_patches_y):\n",
    "                        actual_width = min(patch_size[0], width - x * patch_size[0])\n",
    "                        actual_height = min(patch_size[1], height - y * patch_size[1])\n",
    "                        \n",
    "                        if actual_width * actual_height >= min_patch_area:\n",
    "                            total_patches += 1\n",
    "                \n",
    "                try:\n",
    "                    patches_count[image_id][class_folder] += max(1, total_patches)\n",
    "                except:\n",
    "                    patches_count[image_id][class_folder] = max(1, total_patches)\n",
    "\n",
    "                image_np = np.array(image)\n",
    "                hsv_image = cv2.cvtColor(image_np, cv2.COLOR_RGB2HSV)\n",
    "                \n",
    "                hist = cv2.calcHist([hsv_image], [0], None, [50], [0, 180])\n",
    "                hist = cv2.normalize(hist, hist).flatten()\n",
    "                color_histograms[class_folder][image_id] = hist\n",
    "\n",
    "print(\"Nb of patches by images by categories :\")\n",
    "for image_id, class_counts in patches_count.items():\n",
    "    print(f\"Image {image_id} :\")\n",
    "    for class_folder, count in class_counts.items():\n",
    "        print(f\"  - {class_folder} : {count} patches\")\n",
    "\n",
    "\n",
    "for class_folder in set(class_folder for counts in patches_count.values() for class_folder in counts):\n",
    "    image_ids = sorted(patches_count.keys())\n",
    "    patch_counts = [patches_count[image_id].get(class_folder, 0) for image_id in image_ids]\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(range(len(image_ids)), patch_counts, tick_label=image_ids)\n",
    "    plt.xlabel(\"Indice des images\")\n",
    "    plt.ylabel(\"Nombre de patches\")\n",
    "    plt.title(f\"Nombre de patches par image pour la classe {class_folder}\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# for class_folder, histograms in color_histograms.items():\n",
    "#     image_ids = list(histograms.keys())\n",
    "#     num_images = len(image_ids)\n",
    "#     similarity_matrix = np.zeros((num_images, num_images))\n",
    "    \n",
    "#     for i in range(num_images):\n",
    "#         for j in range(num_images):\n",
    "#             similarity_matrix[i, j] = jensenshannon(histograms[image_ids[i]], histograms[image_ids[j]])\n",
    "    \n",
    "#     plt.figure(figsize=(8, 6))\n",
    "#     plt.imshow(similarity_matrix, cmap='hot', interpolation='nearest')\n",
    "#     plt.colorbar(label=\"Distance de Jensen-Shannon\")\n",
    "#     plt.xticks(range(num_images), image_ids, rotation=90)\n",
    "#     plt.yticks(range(num_images), image_ids)\n",
    "#     plt.title(f\"Similarité des couleurs pour la classe {class_folder}\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166a918e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans, SpectralClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "\n",
    "# Fix for Windows MKL memory leak warning\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "patch_size = (224, 224)\n",
    "min_patch_area_ratio = 0.6\n",
    "possible_clusters = range(2, 10)\n",
    "use_rgb = False\n",
    "\n",
    "def clustering_methods_analysis(class_folder, save_clusters):\n",
    "    image_id_mapping = defaultdict(list)\n",
    "    image_patch_counts = defaultdict(int)\n",
    "    color_histograms = {}\n",
    "    image_paths = {}\n",
    "    dic_clusters = {}\n",
    "    dic_clusters_patches = {}\n",
    "\n",
    "    for classe in classes:\n",
    "        dic_clusters[classe] = {}\n",
    "        dic_clusters_patches[classe] = {}\n",
    "\n",
    "    class_folder_path = os.path.join(label_path, class_folder)\n",
    "    if os.path.isdir(class_folder_path):\n",
    "        for image_name in os.listdir(class_folder_path):\n",
    "            if image_name.lower().endswith(('png', 'jpg', 'jpeg')):\n",
    "                image_id = image_name.split('_')[2].split('.')[0]\n",
    "                image_path = os.path.join(class_folder_path, image_name)\n",
    "\n",
    "                image = Image.open(image_path).convert(\"RGB\")\n",
    "                image_np = np.array(image)\n",
    "                height, width = image_np.shape[:2]\n",
    "                patch_count = max(1,(width // patch_size[0])) * max(1,(height // patch_size[1]))\n",
    "                image_patch_counts[image_id] += patch_count\n",
    "\n",
    "                if use_rgb:\n",
    "                    hist = cv2.calcHist([image_np], [0, 1, 2], None, [50, 50, 50], [0, 256, 0, 256, 0, 256])\n",
    "                else:\n",
    "                    hsv_image = cv2.cvtColor(image_np, cv2.COLOR_RGB2HSV)\n",
    "                    hist = cv2.calcHist([hsv_image], [0], None, [50], [0, 180])\n",
    "\n",
    "                hist = cv2.normalize(hist, hist).flatten()\n",
    "\n",
    "                color_histograms[image_path] = hist\n",
    "                image_paths[image_path] = image_path\n",
    "                image_id_mapping[image_id].append(image_path)\n",
    "\n",
    "    image_ids = list(color_histograms.keys())\n",
    "    hist_matrix = np.array([color_histograms[i] for i in image_ids])\n",
    "\n",
    "    def check_cluster_constraints(clusters, image_ids, min_image_id_count=3):\n",
    "        cluster_image_ids = defaultdict(set)\n",
    "        for img_idx, cluster in enumerate(clusters):\n",
    "            img_path = image_ids[img_idx]\n",
    "            image_id = [key for key, val in image_id_mapping.items() if img_path in val][0]\n",
    "            cluster_image_ids[cluster].add(image_id)\n",
    "\n",
    "        return all(len(image_ids) >= min_image_id_count for image_ids in cluster_image_ids.values())\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def save_clusters_result(clusters, image_paths, image_ids, class_folder, method, num_clusters, folder_path=\"cluster_analysis_results\"):\n",
    "    \n",
    "        # results dic\n",
    "        result = {\n",
    "            \"method\": method,\n",
    "            \"num_clusters\": num_clusters,\n",
    "            \"class\": class_folder,\n",
    "            \"clusters\": {}\n",
    "        }\n",
    "\n",
    "        \n",
    "        unique_clusters = set(clusters)\n",
    "        for cluster in unique_clusters:\n",
    "            result[\"clusters\"][str(cluster)] = [\n",
    "                image_paths[image_ids[i]] for i in range(len(image_ids)) if clusters[i] == cluster\n",
    "            ]\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "        file_name = f\"{method}_{class_folder}_{num_clusters}.json\"\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "        with open(file_path, \"w\") as f:\n",
    "            json.dump(result, f, indent=4)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # Distance matrix\n",
    "    distance_matrix = np.zeros((len(image_ids), len(image_ids)))\n",
    "    for i, j in combinations(range(len(image_ids)), 2):\n",
    "        dist = jensenshannon(hist_matrix[i], hist_matrix[j])\n",
    "        distance_matrix[i, j] = distance_matrix[j, i] = dist\n",
    "\n",
    "    best_scores_hist_matrix = {'AgglomerativeClustering': [], 'AgglomerativeClusteringBase': [],\n",
    "                            'KMeans++': [], 'KMeans++Base': [], \n",
    "                            'SpectralClustering': [], 'SpectralClusteringBase': []}\n",
    "\n",
    "    best_scores_distance_matrix = {'AgglomerativeClustering': [], 'AgglomerativeClusteringBase': [],\n",
    "                                'KMeans++': [], 'KMeans++Base': [], \n",
    "                                'SpectralClustering': [], 'SpectralClusteringBase': []}\n",
    "\n",
    "    for num_clusters in possible_clusters:\n",
    "        print(f\"Testing with {num_clusters} clusters\")\n",
    "        \n",
    "        agglomerative = AgglomerativeClustering(n_clusters=num_clusters, metric=\"precomputed\", linkage=\"average\")\n",
    "        agglomerative_clusters = agglomerative.fit_predict(distance_matrix)\n",
    "        if save_clusters:\n",
    "            save_clusters_result(agglomerative_clusters, image_paths, image_ids, class_folder, \"AgglomerativeClustering\",num_clusters)\n",
    "\n",
    "        if check_cluster_constraints(agglomerative_clusters, image_ids):\n",
    "            best_scores_distance_matrix['AgglomerativeClustering'].append(silhouette_score(distance_matrix, agglomerative_clusters, metric=\"precomputed\"))\n",
    "            best_scores_hist_matrix['AgglomerativeClustering'].append(silhouette_score(hist_matrix, agglomerative_clusters))\n",
    "        else:\n",
    "            best_scores_distance_matrix['AgglomerativeClustering'].append(np.NaN)\n",
    "            best_scores_hist_matrix['AgglomerativeClustering'].append(np.NaN)\n",
    "        \n",
    "\n",
    "        agglomerative_base = AgglomerativeClustering(n_clusters=num_clusters, linkage=\"average\")\n",
    "        agglomerative_base_clusters = agglomerative_base.fit_predict(hist_matrix)\n",
    "        if save_clusters:\n",
    "            save_clusters_result(agglomerative_base_clusters, image_paths, image_ids, class_folder, \"AgglomerativeClusteringBase\",num_clusters)\n",
    "\n",
    "        if check_cluster_constraints(agglomerative_base_clusters, image_ids):\n",
    "            best_scores_hist_matrix['AgglomerativeClusteringBase'].append(silhouette_score(hist_matrix, agglomerative_base_clusters))\n",
    "            best_scores_distance_matrix['AgglomerativeClusteringBase'].append(silhouette_score(distance_matrix, agglomerative_base_clusters, metric=\"precomputed\"))\n",
    "        else:\n",
    "            best_scores_hist_matrix['AgglomerativeClusteringBase'].append(np.NaN)\n",
    "            best_scores_distance_matrix['AgglomerativeClusteringBase'].append(np.NaN)\n",
    "\n",
    "\n",
    "        # KMeans++ with histograms\n",
    "        kmeans = KMeans(n_clusters=num_clusters, init='k-means++', random_state=42)\n",
    "        kmeans.fit(hist_matrix)\n",
    "        kmeans_clusters = kmeans.labels_\n",
    "        if save_clusters:\n",
    "            save_clusters_result(kmeans_clusters, image_paths, image_ids, class_folder, \"kmeansBase\",num_clusters)\n",
    "\n",
    "        if check_cluster_constraints(kmeans_clusters, image_ids):\n",
    "            best_scores_hist_matrix['KMeans++Base'].append(silhouette_score(hist_matrix, kmeans_clusters))\n",
    "            best_scores_distance_matrix['KMeans++Base'].append(silhouette_score(distance_matrix, kmeans_clusters, metric=\"precomputed\"))\n",
    "        else:\n",
    "            best_scores_hist_matrix['KMeans++Base'].append(np.NaN)\n",
    "            best_scores_distance_matrix['KMeans++Base'].append(np.NaN)\n",
    "\n",
    "        # KMeans++ matrix distances\n",
    "        kmeans_distance = KMeans(n_clusters=num_clusters, init='k-means++', random_state=42)\n",
    "        kmeans_distance.fit(distance_matrix)\n",
    "        kmeans_distance_clusters = kmeans_distance.labels_\n",
    "        if save_clusters:\n",
    "            save_clusters_result(kmeans_distance_clusters, image_paths, image_ids, class_folder, \"kmeans\",num_clusters)\n",
    "\n",
    "        if check_cluster_constraints(kmeans_distance_clusters, image_ids):\n",
    "            best_scores_hist_matrix['KMeans++'].append(silhouette_score(hist_matrix, kmeans_distance_clusters))\n",
    "            best_scores_distance_matrix['KMeans++'].append(silhouette_score(distance_matrix, kmeans_distance_clusters, metric=\"precomputed\"))\n",
    "        else:\n",
    "            best_scores_hist_matrix['KMeans++'].append(np.NaN)\n",
    "            best_scores_distance_matrix['KMeans++'].append(np.NaN)\n",
    "\n",
    "        # Spectral Clustering matrix distances\n",
    "        spectral = SpectralClustering(n_clusters=num_clusters, affinity=\"precomputed\", random_state=42)\n",
    "        spectral_clusters = spectral.fit_predict(distance_matrix)\n",
    "        if save_clusters:\n",
    "            save_clusters_result(spectral_clusters, image_paths, image_ids, class_folder, \"spectral\",num_clusters)\n",
    "\n",
    "        if check_cluster_constraints(spectral_clusters, image_ids):\n",
    "            best_scores_hist_matrix['SpectralClustering'].append(silhouette_score(hist_matrix, spectral_clusters))\n",
    "            best_scores_distance_matrix['SpectralClustering'].append(silhouette_score(distance_matrix, spectral_clusters, metric=\"precomputed\"))\n",
    "        else:\n",
    "            best_scores_hist_matrix['SpectralClustering'].append(np.NaN)\n",
    "            best_scores_distance_matrix['SpectralClustering'].append(np.NaN)\n",
    "\n",
    "        # Spectral Clustering with histograms\n",
    "        spectral_base = SpectralClustering(n_clusters=num_clusters, affinity=\"nearest_neighbors\", random_state=42)\n",
    "        spectral_base_clusters = spectral_base.fit_predict(hist_matrix)\n",
    "        if save_clusters:\n",
    "            save_clusters_result(spectral_base_clusters, image_paths, image_ids, class_folder, \"spectralBase\",num_clusters)\n",
    "\n",
    "\n",
    "        if check_cluster_constraints(spectral_base_clusters, image_ids):\n",
    "            best_scores_hist_matrix['SpectralClusteringBase'].append(silhouette_score(hist_matrix, spectral_base_clusters))\n",
    "            best_scores_distance_matrix['SpectralClusteringBase'].append(silhouette_score(distance_matrix, spectral_base_clusters, metric=\"precomputed\"))\n",
    "        else:\n",
    "            best_scores_hist_matrix['SpectralClusteringBase'].append(np.NaN)\n",
    "            best_scores_distance_matrix['SpectralClusteringBase'].append(np.NaN)\n",
    "\n",
    "\n",
    "    methods = best_scores_hist_matrix.keys()\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Graphique 1 : Silouhette score with histogram matrix\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for method in methods:\n",
    "        plt.plot(possible_clusters, best_scores_hist_matrix[method], label=method)\n",
    "    plt.title(f\"Silhouette Scores {class_folder} (Hist Matrix)\")\n",
    "    plt.xlabel(\"Number of Clusters\")\n",
    "    plt.ylabel(\"Silhouette Score\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Graphique 2 : Silhouette Score distance matrix\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for method in methods:\n",
    "        plt.plot(possible_clusters, best_scores_distance_matrix[method], label=method)\n",
    "    plt.title(f\"Silhouette Scores class {class_folder} (Distance Matrix)\")\n",
    "    plt.xlabel(\"Number of Clusters\")\n",
    "    plt.ylabel(\"Silhouette Score\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"Silouhette_scores_{class_folder}_HSV.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for class_folder in classes:\n",
    "    print(f\"-----Processing for {class_folder} class ... ------\")\n",
    "    clustering_methods_analysis(class_folder, True)\n",
    "\n",
    "\n",
    "def compute_variance(cluster_sizes):\n",
    "    return np.var(cluster_sizes)\n",
    "\n",
    "\n",
    "def compute_entropy(cluster_sizes):\n",
    "    total = sum(cluster_sizes)\n",
    "    probabilities = np.array(cluster_sizes) / total\n",
    "    return -np.sum(probabilities * np.log2(probabilities + 1e-10))\n",
    "\n",
    "def compute_entropy(cluster_sizes):\n",
    "    total = sum(cluster_sizes)\n",
    "    probabilities = np.array(cluster_sizes) / total\n",
    "    return -np.sum(probabilities * np.log2(probabilities + 1e-10))\n",
    "\n",
    "def analyze_cluster_variance(results_folder, patches_count_file):\n",
    "\n",
    "    with open(patches_count_file, 'r') as f:\n",
    "        patches_count = json.load(f)\n",
    "    \n",
    "    data = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "    min_values = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "    \n",
    "\n",
    "    for file in os.listdir(results_folder):\n",
    "        if file.endswith(\".json\") and file != \"patches_count.json\":\n",
    "            file_path = os.path.join(results_folder, file)\n",
    "            with open(file_path, 'r') as f:\n",
    "                cluster_data = json.load(f)\n",
    "            \n",
    "            method = cluster_data[\"method\"]\n",
    "            num_clusters = cluster_data[\"num_clusters\"]\n",
    "            class_label = cluster_data[\"class\"]\n",
    "            \n",
    "            cluster_sizes = []\n",
    "            for cluster in cluster_data[\"clusters\"].values():\n",
    "                cluster_size = sum(patches_count.get(img_path.replace('\\\\','/'), 0) for img_path in cluster)\n",
    "                cluster_sizes.append(cluster_size)\n",
    "            \n",
    "            min_cluster_size = min(cluster_sizes)\n",
    "            print(min_cluster_size, method, num_clusters, class_label)\n",
    "            variance = compute_entropy(cluster_sizes)\n",
    "     \n",
    "            data[class_label][method][num_clusters].append(variance)\n",
    "            min_values[class_label][method][num_clusters].append(min_cluster_size)\n",
    "    \n",
    "\n",
    "    for class_label in data:\n",
    "        for method in data[class_label]:\n",
    "            for num_clusters in data[class_label][method]:\n",
    "                data[class_label][method][num_clusters] = np.mean(data[class_label][method][num_clusters])\n",
    "                min_values[class_label][method][num_clusters] = np.mean(min_values[class_label][method][num_clusters])\n",
    "    \n",
    "    for class_label, methods in data.items():\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for method, variances in methods.items():\n",
    "            sorted_clusters = sorted(variances.keys())\n",
    "            sorted_mins = [min_values[class_label][method][k] for k in sorted_clusters]\n",
    "            for i in range(len(sorted_mins)):\n",
    "                sorted_mins[i]*=(i+2)\n",
    "            \n",
    "            plt.plot(sorted_clusters, sorted_mins, marker='o', label=method)\n",
    "            \n",
    "          \n",
    "        plt.xlabel(\"Number of clusters\")\n",
    "        plt.ylabel(\"Variance du nombre de patches\")\n",
    "        plt.title(f\"Variance des clusters pour la classe {class_label}\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.savefig(f\"entropy_by_classes_{class_label}.pdf\")\n",
    "        plt.show()\n",
    "\n",
    "analyze_cluster_variance(\"cluster_analysis_results\", \"patches_count.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c32c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dash\n",
    "from dash import dcc, html, Input, Output, State\n",
    "import dash_bootstrap_components as dbc\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "import base64\n",
    "import io\n",
    "\n",
    "app = dash.Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n",
    "\n",
    "clustering_methods = [\"AgglomerativeClustering\", \"AgglomerativeClusteringBase\", \"kmeans\", \"kmeansBase\", \"spectral\", \"spectralBase\"]\n",
    "n_clusters_options = list(range(2, 10))\n",
    "\n",
    "folder_path = \"cluster_analysis_results\"\n",
    "\n",
    "app.layout = dbc.Container([\n",
    "    html.H1(\"Cluster visualization\"),\n",
    "    dbc.Row([\n",
    "        dbc.Col([html.Label(\"Clustering method\"),\n",
    "                 dcc.Dropdown(id=\"method-dropdown\", options=[{\"label\": m, \"value\": m} for m in clustering_methods], value=clustering_methods[0])], width=4),\n",
    "        dbc.Col([html.Label(\"Class\"),\n",
    "                 dcc.Dropdown(id=\"class-dropdown\", options=[{\"label\": c, \"value\": c} for c in classes], value=classes[0])], width=4),\n",
    "        dbc.Col([html.Label(\"number of clusters\"),\n",
    "                 dcc.Dropdown(id=\"n-clusters-dropdown\", options=[{\"label\": n, \"value\": n} for n in n_clusters_options], value=n_clusters_options[0])], width=4)\n",
    "    ]),\n",
    "    html.Br(),\n",
    "    dbc.Button(\"Afficher les images\", id=\"load-button\", color=\"primary\", className=\"mb-3\"),\n",
    "    html.Div(id=\"images-container\")\n",
    "])\n",
    "\n",
    "# Callback loading and showing images\n",
    "@app.callback(\n",
    "    Output(\"images-container\", \"children\"),\n",
    "    Input(\"load-button\", \"n_clicks\"),\n",
    "    State(\"method-dropdown\", \"value\"),\n",
    "    State(\"class-dropdown\", \"value\"),\n",
    "    State(\"n-clusters-dropdown\", \"value\")\n",
    ")\n",
    "def update_images(n_clicks, method, class_folder, num_clusters):\n",
    "    if not n_clicks:\n",
    "        return \"Select options and click on the button to show images\"\n",
    "    \n",
    "    file_name = f\"{method}_{class_folder}_{num_clusters}.json\"\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        return f\"No file for {method}, class {class_folder}, {num_clusters} clusters.\"\n",
    "    \n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    images = []\n",
    "    for cluster_id, image_paths in data[\"clusters\"].items():\n",
    "        images.append(html.H3(f\"Cluster {cluster_id}\"))\n",
    "        row_images = []\n",
    "        for img_path in image_paths[:25]:\n",
    "            if os.path.exists(img_path):\n",
    "                with open(img_path, \"rb\") as img_file:\n",
    "                    encoded_img = base64.b64encode(img_file.read()).decode()\n",
    "                    img_element = html.Img(src=f\"data:image/png;base64,{encoded_img}\", style={\"width\": \"150px\", \"margin\": \"5px\"})\n",
    "                    row_images.append(img_element)\n",
    "        images.append(html.Div(row_images, style={\"display\": \"flex\", \"flex-wrap\": \"wrap\"}))\n",
    "    \n",
    "    return images\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run_server(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dddab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans, SpectralClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "def find_optimal_repartition(dic_clusters, dic_clusters_patches, class_folder, dic_clusters_patches_opti, nb_clusters, dic_clusters_opti, dic_methods):\n",
    "    print(f\"----------running {class_folder} class ...... -----------------\")\n",
    "\n",
    "\n",
    "    color_histograms = {}\n",
    "    image_paths = {}\n",
    "    image_id_mapping = defaultdict(list)\n",
    "    image_patch_counts = defaultdict(int)\n",
    "\n",
    "    class_folder_path = os.path.join(label_path, class_folder)\n",
    "    if os.path.isdir(class_folder_path):\n",
    "        for image_name in os.listdir(class_folder_path):\n",
    "            if image_name.lower().endswith(('png', 'jpg', 'jpeg')):\n",
    "                image_id = image_name.split('_')[2].split('.')[0]\n",
    "                image_path = os.path.join(class_folder_path, image_name)\n",
    "\n",
    "                \n",
    "                \n",
    "                image = Image.open(image_path).convert(\"RGB\")\n",
    "                image_np = np.array(image)\n",
    "                height, width = image_np.shape[:2]\n",
    "                patch_count = max(1,(width // patch_size[0])) * max(1,(height // patch_size[1]))\n",
    "                image_patch_counts[image_id] += patch_count\n",
    "                dic_clusters_patches_opti[image_path] = {\"image_id\":image_id, \"patches\":patch_count,\"cluster\":np.NaN}\n",
    "                \n",
    "                if use_rgb:\n",
    "                    hist = cv2.calcHist([image_np], [0, 1, 2], None, [50, 50, 50], [0, 256, 0, 256, 0, 256])\n",
    "                else:\n",
    "                    hsv_image = cv2.cvtColor(image_np, cv2.COLOR_RGB2HSV)\n",
    "                    hist = cv2.calcHist([hsv_image], [0], None, [50], [0, 180])\n",
    "                \n",
    "                hist = cv2.normalize(hist, hist).flatten()\n",
    "                \n",
    "                color_histograms[image_path] = hist\n",
    "                image_paths[image_path] = image_path\n",
    "                image_id_mapping[image_id].append(image_path)\n",
    "\n",
    "    image_ids = list(color_histograms.keys())\n",
    "    hist_matrix = np.array([color_histograms[i] for i in image_ids])\n",
    "\n",
    "    def check_cluster_constraints(clusters, image_ids, min_image_id_count=3):\n",
    "        cluster_image_ids = defaultdict(set)\n",
    "        for img_idx, cluster in enumerate(clusters):\n",
    "            img_path = image_ids[img_idx]\n",
    "            image_id = [key for key, val in image_id_mapping.items() if img_path in val][0]\n",
    "            cluster_image_ids[cluster].add(image_id)\n",
    "        \n",
    "        return all(len(image_ids) >= min_image_id_count for image_ids in cluster_image_ids.values())\n",
    "    \n",
    "\n",
    "    distance_matrix = np.zeros((len(image_ids), len(image_ids)))\n",
    "    for i, j in combinations(range(len(image_ids)), 2):\n",
    "        dist = jensenshannon(hist_matrix[i], hist_matrix[j])\n",
    "        distance_matrix[i, j] = distance_matrix[j, i] = dist\n",
    "\n",
    "\n",
    "    method =  dic_methods[class_folder][\"method\"]\n",
    "    num_clusters = dic_methods[class_folder][\"num_clusters\"]\n",
    "    print(f\" using {method} for {class_folder} class for {num_clusters} clusters\")\n",
    "\n",
    "\n",
    "    if method == \"AgglomerativeClustering\":\n",
    "    \n",
    "        agglomerative = AgglomerativeClustering(n_clusters=num_clusters, metric=\"precomputed\", linkage=\"average\")\n",
    "        clusters = agglomerative.fit_predict(distance_matrix)\n",
    "\n",
    "    elif method == \"AgglomerativeClusteringBase\":\n",
    "        agglomerative_base = AgglomerativeClustering(n_clusters=num_clusters, linkage=\"average\")\n",
    "        clusters = agglomerative_base.fit_predict(hist_matrix)\n",
    "\n",
    "    elif method ==\"kmeansBase\":\n",
    "        kmeans = KMeans(n_clusters=num_clusters, init='k-means++', random_state=42)\n",
    "        kmeans.fit(hist_matrix)\n",
    "        clusters = kmeans.labels_\n",
    "\n",
    "    elif method == \"kmeans\":\n",
    "        kmeans_distance = KMeans(n_clusters=num_clusters, init='k-means++', random_state=42)\n",
    "        kmeans = kmeans_distance.fit(distance_matrix)\n",
    "        clusters = kmeans.labels_\n",
    "\n",
    "    elif method == \"spectral\":\n",
    "        spectral = SpectralClustering(n_clusters=num_clusters, affinity=\"precomputed\", random_state=42)\n",
    "        clusters = spectral.fit_predict(distance_matrix)\n",
    "    \n",
    "    else:\n",
    "        spectral_base = SpectralClustering(n_clusters=num_clusters, affinity=\"nearest_neighbors\", random_state=42)\n",
    "        clusters = spectral_base.fit_predict(hist_matrix)\n",
    "\n",
    "\n",
    "    for i in range(len(image_ids)):\n",
    "         dic_clusters_patches_opti[image_paths[image_ids[i]]][\"cluster\"] = str(clusters[i] + nb_clusters)\n",
    "\n",
    "\n",
    "\n",
    "    for cluster in clusters:\n",
    "\n",
    "        filtered_images = [image_paths[image_ids[i]] for i in range(len(image_ids)) if clusters[i] == cluster]\n",
    "        dic_clusters_opti[str(cluster + nb_clusters)] = filtered_images\n",
    "     \n",
    "        cluster_image_ids = set(\n",
    "            [key for img in filtered_images for key, val in image_id_mapping.items() if img in val]\n",
    "        )\n",
    "        l = []\n",
    "        for img_id in cluster_image_ids:\n",
    "            l.append(image_patch_counts[img_id])\n",
    "\n",
    "        dic_clusters[class_folder][cluster] = list(cluster_image_ids)\n",
    "        dic_clusters_patches[class_folder][cluster] = l.copy()\n",
    "\n",
    "    return num_clusters\n",
    "\n",
    "\n",
    "# Fix for Windows MKL memory leak warning\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "\n",
    "patch_size = (224, 224)\n",
    "min_patch_area_ratio = 0.6\n",
    "possible_clusters = range(2, 6)\n",
    "use_rgb = False\n",
    "\n",
    "\n",
    "dic_clusters = {}\n",
    "dic_clusters_patches = {}\n",
    "dic_clusters_opti = {}\n",
    "dic_clusters_patches_opti = {}\n",
    "dic_methods = {\"1\" : {\"method\":\"kmeansBase\", \"num_clusters\": 2},\n",
    "               \"2\":{\"method\":\"kmeans\", \"num_clusters\": 4},\n",
    "               \"3\":{\"method\":\"kmeans\", \"num_clusters\": 5},\n",
    "               \"41\":{\"method\":\"spectralBase\", \"num_clusters\": 3},\n",
    "               \"42\":{\"method\":\"kmeans\", \"num_clusters\": 3},\n",
    "               \"51\":{\"method\":\"kmeansBase\", \"num_clusters\": 3},\n",
    "               \"52\":{\"method\":\"kmeansBase\", \"num_clusters\": 3},\n",
    "               \"54\":{\"method\":\"kmeansBase\", \"num_clusters\": 3},\n",
    "               \"57\":{\"method\":\"kmeansBase\", \"num_clusters\": 3}}\n",
    "\n",
    "\n",
    "if label_path == \"labels_low_res/\":\n",
    "    dic_methods = {\"1\" : {\"method\":\"kmeansBase\", \"num_clusters\": 2},\n",
    "               \"2\":{\"method\":\"kmeans\", \"num_clusters\": 4},\n",
    "               \"3\":{\"method\":\"kmeans\", \"num_clusters\": 5},\n",
    "               \"41\":{\"method\":\"spectralBase\", \"num_clusters\": 3},\n",
    "               \"5\":{\"method\":\"kmeansBase\", \"num_clusters\": 3}}\n",
    "\n",
    "\n",
    "for classe in classes:\n",
    "    dic_clusters[classe] = {}\n",
    "    dic_clusters_patches[classe] = {}\n",
    "   \n",
    "nb_clusters = 0\n",
    "for class_folder in classes:\n",
    "    nb_clusters += find_optimal_repartition(dic_clusters, dic_clusters_patches, class_folder, dic_clusters_patches_opti, nb_clusters, dic_clusters_opti, dic_methods)\n",
    "    \n",
    "\n",
    "def normalize_path(path):\n",
    "    return os.path.normpath(path).replace(\"\\\\\", \"/\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "images = {normalize_path(k): v for k, v in dic_clusters_patches_opti.items()}\n",
    "clusters = {k: [normalize_path(p) for p in v] for k, v in dic_clusters_opti.items()}\n",
    "\n",
    "save_dict(images,\"images.json\")\n",
    "save_dict(clusters, \"clusters.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0e5470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pulp as lp\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def repartition_optimization(images, clusters, random_opti = ''):\n",
    "    splits = ['train', 'val', 'test']\n",
    "    model = lp.LpProblem(\"Image_Split_Optimization\", lp.LpMinimize)\n",
    "\n",
    "    x = {(i, s): lp.LpVariable(f\"x_{i}_{s}\", cat=lp.LpBinary) for i in images for s in splits}\n",
    "    P_cs = {(c, s): lp.LpVariable(f\"P_{c}_{s}\", cat=lp.LpContinuous) for c in clusters for s in splits}\n",
    "\n",
    "    for i in images:\n",
    "        model += lp.lpSum(x[i, s] for s in splits) == 1, f\"assign_{i}\"\n",
    "    \n",
    "    if random_opti ==\"random_\":\n",
    "        print(\"==== adding random constraint in optimization    \")\n",
    "    \n",
    "        rdm_constraint = load_dict(f\"repartition_labels.json\")\n",
    "        \n",
    "        for i in images:\n",
    "            \n",
    "            ori_img = i.split('/')[-1].split('_')[-1][:-4]\n",
    "        \n",
    "\n",
    "            split = rdm_constraint[ori_img]\n",
    "            if split == \"test\":\n",
    "                model += x[i,'test'] == 1, f\"assign_random_{i}_test\"\n",
    "                model += x[i,'train'] == 0, f\"assign_random_{i}_train\"\n",
    "                model += x[i,'val'] == 0,  f\"assign_random_{i}_val\"\n",
    "            else:\n",
    "                model += x[i,'test'] == 0, f\"assign_random_{i}_test\"\n",
    "\n",
    "    image_ids = {}\n",
    "    for i, img in images.items():\n",
    "        if img['image_id'] not in image_ids:\n",
    "            image_ids[img['image_id']] = []\n",
    "        image_ids[img['image_id']].append(i)\n",
    "\n",
    "    for id_images in image_ids.values():\n",
    "        for s in splits:\n",
    "            for i in id_images:\n",
    "                for j in id_images:\n",
    "                    model += x[i, s] == x[j, s], f\"same_id_{i}_{j}_{s}\"\n",
    "\n",
    "    for c, image_list in clusters.items():\n",
    "        for s in splits:\n",
    "            model += P_cs[c, s] == lp.lpSum(images[str(i)]['patches'] * x[i, s] for i in image_list), f\"patches_{c}_{s}\"\n",
    "            model += lp.lpSum(x[i, s] for i in image_list) >= 1, f\"min_one_image_{c}_{s}\"\n",
    "\n",
    "    split_ratios = {'train': 0.7, 'val': 0.15, 'test': 0.15}\n",
    "\n",
    "    P_target_cs = {c: {s: sum(images[i]['patches'] for i in clusters[c]) * split_ratios[s] for s in splits} for c in clusters}\n",
    "\n",
    "\n",
    "    error = {(c, s): lp.LpVariable(f\"error_{c}_{s}\", lowBound=0) for c in clusters for s in splits}\n",
    "\n",
    "    for c in clusters:\n",
    "        for s in splits:\n",
    "            model += error[c, s] >= P_cs[c, s] - P_target_cs[c][s]\n",
    "            model += error[c, s] >= -(P_cs[c, s] - P_target_cs[c][s])\n",
    "\n",
    "    objective = lp.lpSum(error[c, s] for c in clusters for s in splits)\n",
    "\n",
    "    model += objective\n",
    "    model.solve(lp.PULP_CBC_CMD(msg=True))\n",
    "\n",
    "\n",
    "    res = {}\n",
    "    results = {}\n",
    "    patch_counts = {s: 0 for s in splits}\n",
    "\n",
    "    images_reparted = {}\n",
    "\n",
    "    if lp.LpStatus[model.status] == \"Optimal\":\n",
    "        for i in images:\n",
    "            class_id = i.split(\"/\")[1]\n",
    "            cluster = images[i][\"cluster\"]\n",
    "            patches = images[i][\"patches\"]\n",
    "            for s in splits:\n",
    "                if x[i, s].varValue > 0.5:\n",
    "                    print(f\"Image {i} -> {s}\")\n",
    "                    images_reparted[i] = {\"cluster\": cluster, \"patches\": patches, \"class_id\": class_id, \"split\": s}\n",
    "                    results.setdefault(class_id, {}).setdefault(cluster, {}).setdefault(s, 0)\n",
    "                    results[class_id][cluster][s] += patches\n",
    "                    \n",
    "                    num_patches = images[i]['patches']\n",
    "                    patch_counts[s] += num_patches\n",
    "                    \n",
    "                    final_img_id = i.split('_')[2].split('.')[0]\n",
    "                    if label_path == \"labels_low_res/\":\n",
    "                        final_img_id = i.split('/')[2].split('.')[0].split('_')[2]\n",
    "                \n",
    "                    if final_img_id in res and res[final_img_id] != s:\n",
    "                        print('PROBLEME')\n",
    "                    else:\n",
    "                        res[final_img_id] = s\n",
    "\n",
    "    \n",
    "    save_dict(res, f\"{random_opti}repartition_labels.json\")\n",
    "    save_dict(images_reparted, f\"{random_opti}image_reparted.json\")\n",
    "    save_dict(results, f\"{random_opti}results.json\")\n",
    "\n",
    "    print(\"Objective value :\", model.objective.value())\n",
    "\n",
    "    total_patches = sum(patch_counts.values())\n",
    "    if total_patches > 0:\n",
    "        patch_proportions = {s: patch_counts[s] / total_patches * 100 for s in splits}\n",
    "    else:\n",
    "        patch_proportions = {s: 0 for s in splits}\n",
    "\n",
    "    print(\"\\n Nb patches by split :\")\n",
    "    for s in splits:\n",
    "        print(f\"{s}: {patch_counts[s]} patches ({patch_proportions[s]:.2f}%)\")\n",
    "\n",
    "\n",
    "    for class_id, clusters in results.items():\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        df = []\n",
    "        \n",
    "        for cluster, splits in clusters.items():\n",
    "            for split, count in splits.items():\n",
    "                df.append({\"Split\": split, \"Patches\": count, \"Cluster\": cluster})\n",
    "        \n",
    "        df = pd.DataFrame(df)\n",
    "        \n",
    "        ax = sns.barplot(x=\"Split\", y=\"Patches\", hue=\"Cluster\", data=df)\n",
    "        \n",
    "        for p in ax.patches:\n",
    "            ax.annotate(f'{int(p.get_height())}', \n",
    "                        (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                        ha='center', va='bottom', fontsize=10, color='black')\n",
    "\n",
    "        plt.title(f\"Distribution des patches pour la classe {class_id}\")\n",
    "        plt.xlabel(\"Split\")\n",
    "        plt.ylabel(\"Nombre de patches\")\n",
    "        plt.legend(title=\"Cluster\")\n",
    "        plt.savefig(f\"{random_opti}repartition_patches_class_{class_id}.png\")\n",
    "        plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "true_opti =\"random_\"\n",
    "# true_opti = \"\"\n",
    "\n",
    "images = load_dict(\"images.json\")\n",
    "clusters = load_dict(\"clusters.json\")\n",
    "\n",
    "\n",
    "if true_opti == \"random_\":\n",
    "    print(\"Performing random optimization repartition ...\")\n",
    "    ###Random optimization\n",
    "    clusters = {}\n",
    "\n",
    "   \n",
    "    for image_path, data in images.items():\n",
    "        # for image_id, patches, cluster in data.values():\n",
    "        cluster = str(classes.index(image_path.split('/')[1]))\n",
    "        images[image_path][\"cluster\"] = cluster\n",
    "        if cluster in clusters:\n",
    "            clusters[cluster] += [image_path]\n",
    "        else:\n",
    "            clusters[cluster] = [image_path]\n",
    "\n",
    "\n",
    "print(images)\n",
    "print(clusters)\n",
    "repartition_optimization(images, clusters, true_opti)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facf12eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def redistribute_patches(results, images_reparted, num_shots_train, num_shots_val, num_shots_test, random_opti=''):\n",
    "    patches_per_image = {}\n",
    "    clusters_distribution = defaultdict(lambda: {'train': {}, 'val': {}, 'test': {}})\n",
    "    \n",
    "    num_shots_dict = {'train': num_shots_train, 'val': num_shots_val, 'test': num_shots_test}\n",
    "    \n",
    "    for class_id, clusters in results.items():\n",
    "        for split in ['train', 'val', 'test']:\n",
    "            num_shots = num_shots_dict[split]\n",
    "            total_patches = sum(cluster_info.get(split, 0) for cluster_info in clusters.values())\n",
    "\n",
    "            cluster_patches = {k: v.get(split, 0) for k, v in clusters.items()}\n",
    "            clusters_distribution[class_id][split] = cluster_patches.copy()\n",
    "\n",
    "            if total_patches == 0:\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            for image_path, image_info in images_reparted.items():\n",
    "                if image_info['class_id'] == class_id and image_info['split'] == split:\n",
    "                    patches_per_image[image_path] = {'patches': image_info['patches'], 'split': split, 'cluster':image_info['cluster']}\n",
    "\n",
    "            if total_patches <= num_shots:\n",
    "                continue\n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "            while sum(cluster_patches.values()) > num_shots:\n",
    "            \n",
    "                max_cluster = max(cluster_patches, key=cluster_patches.get)\n",
    "\n",
    "                images_with_patches = [\n",
    "                    img for img, img_info in images_reparted.items()\n",
    "                    if img_info['class_id'] == class_id and img_info['split'] == split and patches_per_image.get(img, {}).get('patches', 0) > 0\n",
    "                ]\n",
    "\n",
    "                if all(patches_per_image[img]['patches'] == 1 for img in images_with_patches):\n",
    "                    max_image = random.choice(images_with_patches)\n",
    "                else:\n",
    "                    max_image = max(\n",
    "                        (img for img in images_with_patches if images_reparted[img]['cluster'] == max_cluster),\n",
    "                        key=lambda img: patches_per_image[img]['patches'],\n",
    "                        default=None\n",
    "                    )\n",
    "\n",
    "                if not max_image or patches_per_image[max_image]['patches'] == 0:\n",
    "                    print(f\"Impossible de réduire davantage les patches pour la classe {class_id}, split {split}.\")\n",
    "                    break\n",
    "\n",
    "            \n",
    "                patches_per_image[max_image]['patches'] -= 1\n",
    "                cluster_patches[max_cluster] -= 1\n",
    "\n",
    "            clusters_distribution[class_id][split] = cluster_patches.copy()\n",
    "    \n",
    "    plot_clusters_distribution(clusters_distribution,random_opti)\n",
    "    plot_before_after_repartition(results, clusters_distribution, num_shots_dict,random_opti)\n",
    "    # plot_patches_per_image(images_reparted, patches_per_image_before=images,patches_per_image_after=patches_per_image,random_opti=random_opti)\n",
    "    return patches_per_image\n",
    "\n",
    "def plot_clusters_distribution(clusters_distribution, random_opti):\n",
    "    for class_id, splits in clusters_distribution.items():\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "        for ax, (split, clusters) in zip(axes, splits.items()):\n",
    "            print(sum(clusters.values()))\n",
    "            if clusters and sum(clusters.values()) > 0:\n",
    "                ax.bar(clusters.keys(), clusters.values())\n",
    "\n",
    "            \n",
    "            ax.set_xlabel(\"Clusters\")\n",
    "            ax.set_ylabel(\"Number of attributed patches\")\n",
    "            ax.set_title(f\"Patches repartition ({split}) - Class {class_id}\")\n",
    "            ax.set_xticks(list(clusters.keys()))\n",
    "            ax.set_xticklabels(list(clusters.keys()), rotation=45)\n",
    "        plt.savefig(f\"{random_opti}Repartition_des_patches\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_before_after_repartition(results, clusters_distribution, num_shots_dict, random_opti):\n",
    "    \"\"\"\n",
    "    Show patches repartition before and after equalization\n",
    "    \"\"\"\n",
    "    for class_id, clusters in results.items():\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        df = []\n",
    "\n",
    "        num_clusters = len(clusters)\n",
    "\n",
    "        for cluster, splits in clusters.items():\n",
    "            for split, count in splits.items():\n",
    "                df.append({\"Split\": split, \"Patches\": count, \"Cluster\": cluster, \"Status\": \"Avant\"})\n",
    "\n",
    "        for split, splits in clusters_distribution[class_id].items():\n",
    "            for cluster, count in splits.items():\n",
    "                df.append({\"Split\": split, \"Patches\": count, \"Cluster\": cluster, \"Status\": \"Après\"})\n",
    "\n",
    "\n",
    "        df = pd.DataFrame(df)\n",
    "        \n",
    "        df[\"Split\"], df[\"Cluster\"] = df[\"Split\"].astype(str), df[\"Cluster\"].astype(str) \n",
    "        splits = df[\"Split\"].unique()\n",
    "\n",
    "        fig, axes = plt.subplots(1, len(splits), figsize=(15, 5), sharey=True)\n",
    "\n",
    "        for ax, split in zip(axes, splits):\n",
    "            num_shots = num_shots_dict[split]\n",
    "            df_split = df[df[\"Split\"] == split]\n",
    "            sns.barplot(x=\"Cluster\", y=\"Patches\", hue=\"Status\", data=df_split, ax=ax, palette={\"Avant\": \"blue\", \"Après\": \"orange\"})\n",
    "            \n",
    "            ax.axhline(num_shots / df_split[\"Cluster\"].nunique(), color=\"red\", linestyle=\"dashed\", label=\"Rééquilibrage idéal\")\n",
    "            \n",
    "            ax.set_title(f\"Patches repartition ({split})\")\n",
    "            ax.set_xlabel(\"Clusters\")\n",
    "            ax.set_ylabel(\"Number of patches\")\n",
    "            ax.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{random_opti}repartition_patches_avant_apres_class_{class_id}.pdf\")\n",
    "        plt.show()\n",
    "\n",
    "def plot_patches_per_image(images_reparted, patches_per_image_before, patches_per_image_after, random_opti):\n",
    "    df = []\n",
    "    \n",
    "    for image_path, image_info in images_reparted.items():\n",
    "        cluster = image_info.get(\"cluster\")\n",
    "        split = image_info.get(\"split\")\n",
    "        \n",
    "        before = patches_per_image_before.get(image_path, {}).get(\"patches\", 0)\n",
    "        after = patches_per_image_after.get(image_path, {}).get(\"patches\", 0)\n",
    "        \n",
    "        if before > 0:\n",
    "            df.append({\"Image\": image_path, \"Patches\": before, \"Cluster\": cluster, \"Split\": split, \"Status\": \"Avant\"})\n",
    "            df.append({\"Image\": image_path, \"Patches\": after, \"Cluster\": cluster, \"Split\": split, \"Status\": \"Après\"})\n",
    "    \n",
    "    df = pd.DataFrame(df)\n",
    "    \n",
    "    clusters = sorted(df[\"Cluster\"].unique())\n",
    "    \n",
    "    for cluster in clusters:\n",
    "        df_cluster = df[df[\"Cluster\"] == cluster]\n",
    "        if df_cluster.empty:\n",
    "            continue\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 5), sharey=True)\n",
    "        \n",
    "        for j, split in enumerate([\"train\", \"val\", \"test\"]):\n",
    "            ax = axes[j]\n",
    "            df_cluster_split = df_cluster[df_cluster[\"Split\"] == split]\n",
    "            \n",
    "            if not df_cluster_split.empty:\n",
    "                sns.barplot(x=\"Image\", y=\"Patches\", hue=\"Status\", data=df_cluster_split, ax=ax, palette={\"Avant\": \"blue\", \"Après\": \"orange\"})\n",
    "                ax.set_yscale(\"log\") \n",
    "                ax.set_title(f\"Cluster {cluster} - {split}\")\n",
    "                ax.set_xlabel(\"Image Path\")\n",
    "                ax.set_ylabel(\"Number of patches (log scale)\")\n",
    "                ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "                ax.legend(title=\"State\")\n",
    "            else:\n",
    "                ax.set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{random_opti}patches_per_image_cluster_{cluster}.png\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "random_opti = 'random_'\n",
    "# random_opti = ''\n",
    "images_reparted = load_dict(f\"{random_opti}image_reparted.json\")\n",
    "results = load_dict(f\"{random_opti}results.json\")\n",
    "\n",
    "# Patches Redistribution\n",
    "patches_per_image = redistribute_patches(results, images_reparted, 60000, 10000, 10000, random_opti)\n",
    "\n",
    "save_dict(patches_per_image, \"patches_per_image.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9473a919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "def setup_folders():\n",
    "    for folder in [\"train\", \"val\", \"test\"]:\n",
    "        if label_path == \"labels_low_res/\":\n",
    "            folder +=  \"_low_res\" \n",
    "        if os.path.exists(folder):\n",
    "            shutil.rmtree(folder)   \n",
    "        os.makedirs(folder)\n",
    "\n",
    "def extract_patches(image_path, patch_size=(224, 224), min_patch_area_ratio=0.6):\n",
    "\n",
    "    image = Image.open(image_path)\n",
    "    if image.size[0] < patch_size[0] or image.size[1] < patch_size[1]:\n",
    "        image = ImageOps.pad(image, patch_size, color=(0, 0, 0))\n",
    "    \n",
    "    min_patch_area = patch_size[0] * patch_size[1] * min_patch_area_ratio\n",
    "    patches = []\n",
    "    \n",
    "    for x in range(0, image.size[0], patch_size[0]):\n",
    "        for y in range(0, image.size[1], patch_size[1]):\n",
    "            actual_width = min(patch_size[0], image.size[0] - x)\n",
    "            actual_height = min(patch_size[1], image.size[1] - y)\n",
    "            \n",
    "            if actual_width * actual_height >= min_patch_area:\n",
    "                patch = image.crop((x, y, x + actual_width, y + actual_height))\n",
    "                patches.append(patch)\n",
    "    \n",
    "    return patches\n",
    "\n",
    "def save_patches(patches_per_image, patch_size=(224, 224)):\n",
    "    for image_path, info in patches_per_image.items():\n",
    "        num_patches = info[\"patches\"]\n",
    "        split = info[\"split\"]\n",
    "        cluster = info[\"cluster\"]\n",
    "        \n",
    "        patches = extract_patches(image_path, patch_size)\n",
    "       \n",
    "        selected_patches = random.sample(patches, min(num_patches, len(patches)))\n",
    "        \n",
    "        if label_path == \"labels_low_res/\":\n",
    "          \n",
    "            split +=  \"_low_res\" \n",
    "        for idx, patch in enumerate(selected_patches):\n",
    "            patch_filename = f\"{os.path.basename(image_path).split('.')[0]}_patch{idx}.png\"\n",
    "            patch_path = os.path.join(split, patch_filename)\n",
    "            patch.save(patch_path)\n",
    "\n",
    "def main(patches_per_image):\n",
    "    setup_folders()\n",
    "    save_patches(patches_per_image)\n",
    "    print(\"Patches generation over\")\n",
    "\n",
    "\n",
    "main(load_dict(\"patches_per_image.json\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb39860-956d-4f94-ba55-8c4f036797a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Patch:\n",
    "    def __init__(self, x, y, prediction, embedding, probabilities):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.prediction = prediction \n",
    "        self.embedding = embedding\n",
    "        self.probabilities = probabilities \n",
    "def create_patches_from_parquet(file_path):\n",
    "    patches = []\n",
    "    df = pd.read_parquet(file_path)\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        patch = Patch(\n",
    "            x=row['x'],\n",
    "            y=row['y'],\n",
    "            prediction=row['prediction'],\n",
    "            embedding=row['embedding'],  \n",
    "            probabilities=row['probabilities']\n",
    "        )\n",
    "        patches.append(patch)\n",
    "    \n",
    "    return patches\n",
    "\n",
    "def load_patches_from_folder(folder_path):\n",
    "    all_patches = []\n",
    "    \n",
    "    for file_name in os.listdir(folder_path):\n",
    "    \n",
    "        if file_name.endswith('.parquet'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            patches = create_patches_from_parquet(file_path)\n",
    "            all_patches.extend(patches)\n",
    "            print(f\"{len(patches)} patches loaded from {file_name}\")\n",
    "    \n",
    "    print(f\"Nb total of loaded patches : {len(all_patches)}\")\n",
    "    return all_patches\n",
    "\n",
    "folder_path = \"../../Cytology-fine-tuning/wsi_results_high_res_cluster/11C01217/\"  # ➡️ REMPLACER par le chemin réel\n",
    "\n",
    "patches = load_patches_from_folder(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64edd444-7eba-4a31-889b-12632d0e3c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "classes = [\"1\",\"2\",\"3\",\"41\",\"42\",\"51\",\"52\",\"54\",\"57\"]\n",
    "\n",
    "def flatten_embeddings(patches):\n",
    "    print(patches[0].embedding.shape)\n",
    "    return np.array([np.array(patch.embedding) for patch in patches])\n",
    "\n",
    "def entropy(probs):\n",
    "    return -np.sum(probs * np.log(probs + 1e-9), axis=-1)\n",
    "\n",
    "def visualize_propagated_labels(patches, labels, filename):\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    unique_labels = np.unique(labels)\n",
    "    n_classes = len(unique_labels)\n",
    "    cmap = plt.get_cmap('tab20', n_classes)\n",
    "    uncertainties = np.array([entropy(patch.probabilities) for patch in patches])\n",
    "    probabilities_array = np.array([patch.probabilities for patch in patches])\n",
    "\n",
    "    if uncertainties.max() - uncertainties.min() == 0:\n",
    "        alpha_values = np.ones_like(uncertainties)\n",
    "    else:\n",
    "        alpha_values = 1 - (uncertainties - uncertainties.min()) / (uncertainties.max() - uncertainties.min() + 1e-6)\n",
    "\n",
    "    scatter = plt.scatter(\n",
    "        [patch.x for patch in patches], \n",
    "        [patch.y for patch in patches], \n",
    "        c=labels, cmap=cmap, s=10\n",
    "    )\n",
    "\n",
    "    plt.colorbar(scatter, label='Propagated label', ticks=range(len(classes))).ax.set_yticklabels(classes)\n",
    "    plt.title(\"Label propagation\")\n",
    "    plt.xlabel(\"X position\")\n",
    "    plt.ylabel(\"Y position\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    scatter_entropy = plt.scatter(\n",
    "        [patch.x for patch in patches], \n",
    "        [patch.y for patch in patches], \n",
    "        c=alpha_values, cmap='viridis', s=10\n",
    "    )\n",
    "\n",
    "    plt.colorbar(scatter_entropy, label='Prediction entropy')\n",
    "    plt.title(\"Entropy map of patch predictions\")\n",
    "    plt.xlabel(\"X position\")\n",
    "    plt.ylabel(\"Y position\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "\n",
    "    height = 191\n",
    "    width = 412\n",
    "\n",
    "    if len(labels) != height * width:\n",
    "        raise ValueError(f\"Mismatch in number of labels: {height}x{width}\")\n",
    "\n",
    "    max_prob_indices = np.argmax(probabilities_array.reshape((78692,9)), axis=-1) \n",
    "    label_grid = probabilities_array.reshape((width, height, 9)) \n",
    "    grad_x = np.gradient(label_grid, axis=1)\n",
    "    grad_y = np.gradient(label_grid, axis=0)  \n",
    "    gradient_magnitude = np.sqrt(np.sum(abs(grad_x), axis=-1) + np.sum(abs(grad_y), axis=-1))\n",
    "\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    scatter_grad = plt.scatter(\n",
    "        [patch.x for patch in patches], \n",
    "        [patch.y for patch in patches], \n",
    "        c=gradient_magnitude, cmap='viridis', s=10\n",
    "    )\n",
    "\n",
    "    plt.colorbar(scatter_grad, label='Prediction entropy error')\n",
    "    plt.title(\"Entropy map of patches\")\n",
    "    plt.xlabel(\"X position\")\n",
    "    plt.ylabel(\"Y position\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "\n",
    "    label_grid = probabilities_array.reshape((width, height, 9))\n",
    "    label_grid = label_grid[:, ::-1, :]\n",
    "\n",
    "    grad_x = np.gradient(label_grid, axis=1)\n",
    "    grad_y = np.gradient(label_grid, axis=0) \n",
    "    gradient_magnitude = np.sqrt(np.sum(grad_x**2, axis=-1) + np.sum(grad_y**2, axis=-1))\n",
    "\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    plt.imshow(gradient_magnitude, cmap='coolwarm', interpolation='nearest')\n",
    "    plt.colorbar(label=\"Gradient magnitude\")\n",
    "    plt.title(\"Gradient map of propagated labels\")\n",
    "    plt.xlabel(\"X position\")\n",
    "    plt.ylabel(\"Y position\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "\n",
    "def plot_gradient_map_per_class(patches, class_index, class_name, threshold=0.1, radius=225):\n",
    "    probabilities_array = np.array([patch.probabilities[class_index] for patch in patches])\n",
    "    threshold = [0.9,0.92,0.92,0.82,0.5,0.5,0.5,0.5,0.5]\n",
    "    x_coords = np.array([patch.x for patch in patches])\n",
    "    y_coords = np.array([patch.y for patch in patches])\n",
    "    grad_x = np.gradient(probabilities_array)\n",
    "    maxima_indices = np.where((grad_x[:-1] > 0) & (grad_x[1:] < 0) & (probabilities_array[1:] > threshold[class_index]))[0] + 1\n",
    "    non_maxima_indices = np.setdiff1d(np.where(probabilities_array > threshold[class_index])[0], maxima_indices)\n",
    "\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    scatter = plt.scatter(x_coords, y_coords, c=grad_x, cmap='viridis', s=10, label='Gradients')\n",
    "    plt.scatter(x_coords[maxima_indices], y_coords[maxima_indices], c='red', s=50, marker='*', label='Local maximas')\n",
    "    plt.colorbar(scatter, label=f'Probability Gradient - Class {class_name}')\n",
    "    plt.title(f'Gradient Map - Class {class_name}')\n",
    "    plt.xlabel(\"X position\")\n",
    "    plt.ylabel(\"Y position\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"grad_map_{class_index}.png\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(np.zeros(len(maxima_indices)), probabilities_array[maxima_indices], color='red', label='Local maximas', alpha=0.7)\n",
    "    plt.scatter(np.ones(len(non_maxima_indices)), probabilities_array[non_maxima_indices], color='blue', label='Non-maximas', alpha=0.7)\n",
    "    plt.xticks([0, 1], ['Maximas', 'Non-maximas'])\n",
    "    plt.ylabel(\"Prediction probability\")\n",
    "    plt.title(f'Probability comparison - Class {class_name}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_updated_predictions(patches, class_index, class_name):\n",
    "    probabilities_array = np.array([patch.probabilities[0][class_index] for patch in patches])\n",
    "    x_coords = np.array([patch.x for patch in patches])\n",
    "    y_coords = np.array([patch.y for patch in patches])\n",
    "\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    scatter = plt.scatter(x_coords, y_coords, c=probabilities_array, cmap='coolwarm', s=10)\n",
    "    plt.colorbar(scatter, label=f'Probability after propagation - Class {class_name}')\n",
    "    plt.title(f'Post-propagation Probability Map - Class {class_name}')\n",
    "    plt.xlabel(\"X position\")\n",
    "    plt.ylabel(\"Y position\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "\n",
    "def compute_multiclass_gradients(patches, num_classes):\n",
    "    gradients = np.zeros((len(patches), num_classes))\n",
    "    for c in range(num_classes):\n",
    "        probabilities = np.array([patch.probabilities[c] for patch in patches])\n",
    "        gradients[:, c] = np.gradient(probabilities)\n",
    "    return gradients\n",
    "\n",
    "def find_multiclass_maxima(gradients, probabilities, threshold=0.0):\n",
    "    maxima_indices = []\n",
    "    for c in range(gradients.shape[1]):\n",
    "        for i in range(1, len(probabilities) - 1):\n",
    "            if gradients[i - 1, c] > 0 and gradients[i + 1, c] < 0 and probabilities[i, c] > threshold[c]:\n",
    "                maxima_indices.append(i)\n",
    "    return np.array(maxima_indices, dtype=int)\n",
    "\n",
    "def propagate_labels_multiclass(patches, maxima_indices, num_classes, radius, lambda_factor=0.7):\n",
    "    interactions = {\n",
    "        0: [1, 2],\n",
    "        1: [0, 1, 2, 5],\n",
    "        2: [0, 1, 2, 5],\n",
    "        3: [2, 3],\n",
    "        4: [3],\n",
    "        5: [0],\n",
    "        6: [5],\n",
    "        7: [0],\n",
    "        8: [0]\n",
    "    }\n",
    "\n",
    "    x_coords = np.array([patch.x for patch in patches])\n",
    "    y_coords = np.array([patch.y for patch in patches])\n",
    "    probabilities = np.array([patch.probabilities for patch in patches])\n",
    "    tree = KDTree(np.column_stack((x_coords, y_coords)))\n",
    "\n",
    "    for idx in maxima_indices:\n",
    "        neighbors = tree.query_ball_point([x_coords[idx], y_coords[idx]], r=radius)\n",
    "        for neighbor in neighbors:\n",
    "            pred_idx = np.argmax(probabilities[idx])\n",
    "            pred_neighbor_idx = np.argmax(probabilities[neighbor])\n",
    "            if pred_neighbor_idx in interactions.get(pred_idx, []):\n",
    "                if np.max(probabilities[neighbor]) < np.max(probabilities[idx]):\n",
    "                    probabilities[neighbor] += lambda_factor * (probabilities[idx])\n",
    "                    patches[neighbor].probabilities = probabilities[neighbor]\n",
    "    return patches\n",
    "\n",
    "def plot_results(patches, maxima_indices, class_names):\n",
    "    x_coords = np.array([patch.x for patch in patches])\n",
    "    y_coords = np.array([patch.y for patch in patches])\n",
    "    probabilities = np.array([patch.probabilities for patch in patches])\n",
    "\n",
    "    for c, class_name in enumerate(class_names):\n",
    "        plt.figure(figsize=(16, 10))\n",
    "        plt.scatter(x_coords, y_coords, c=probabilities[:, c], cmap='viridis', s=10, label=f'Class {class_name}')\n",
    "        plt.scatter(x_coords[maxima_indices], y_coords[maxima_indices], c='red', s=50, marker='*', label='Maximas')\n",
    "        plt.colorbar(label=f'Probability - Class {class_name}')\n",
    "        plt.title(f'Post-propagation Map - Class {class_name}')\n",
    "        plt.xlabel(\"X position\")\n",
    "        plt.ylabel(\"Y position\")\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "num_classes = len(classes)\n",
    "radius = 600\n",
    "\n",
    "threshold = 0.5\n",
    "threshold = [0.9,0.92,0.92,0.82,0.85,0.78,0.5,0.8,0.75]\n",
    "threshold = [0.9,0.92,0.92,0.82,0.5,0.5,0.5,0.5,0.5]\n",
    "labels = np.array([patch.prediction for patch in patches])\n",
    "\n",
    "visualize_propagated_labels(patches, labels, \"_non_propagated_\")\n",
    "gradients = compute_multiclass_gradients(patches, num_classes)\n",
    "probabilities = np.array([patch.probabilities for patch in patches])\n",
    "maxima_indices = find_multiclass_maxima(gradients, probabilities, threshold)\n",
    "\n",
    "patches = propagate_labels_multiclass(patches, maxima_indices, num_classes, radius)\n",
    "\n",
    "labels = np.array([np.argmax(patch.probabilities) for patch in patches])\n",
    "\n",
    "unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "percentages = (counts / len(labels)) * 100\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "bars = ax.bar(classes, percentages, color='skyblue')\n",
    "\n",
    "\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, yval + 1, f'{yval:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "\n",
    "ax.set_xlabel('Classes')\n",
    "ax.set_ylabel('Pourcentage')\n",
    "ax.set_title('Labels repartition in pourcentages')\n",
    "ax.set_xticks(np.arange(9))\n",
    "ax.set_xticklabels([str(i) for i in range(9)])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "import json\n",
    "\n",
    "changed_labels_list = [\n",
    "    {\"x\": patche.x, \"y\": patche.y, \"pred\": patche.prediction, \"label\": int(label)}\n",
    "    for patche, label in zip(patches, labels) if label != patche.prediction\n",
    "]\n",
    "\n",
    "json_path = \"changed_labels.json\"\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump(changed_labels_list, f)\n",
    "\n",
    "\n",
    "\n",
    "visualize_propagated_labels(patches, labels, \"_propagated_\")\n",
    "\n",
    "\n",
    "# # Propagate labels according to confidence of predictions\n",
    "# labels_propagated, labels = propagate_labels(patches, threshold=0.0)\n",
    "#     \n",
    "# labels, mask_confident, probabilities  = identify_confident_and_uncertain_labels(patches, 0.0)\n",
    "# # sampled_embeddings, sampled_labels = sample_balanced_embeddings(patches, labels, sample_size=1500)\n",
    "# # print(sampled_embeddings.shape, sampled_labels.shape)\n",
    "\n",
    "# # visualize_propagated_labels(patches, labels_propagated,\"wsi_labels_propagated.png\")\n",
    "# visualize_propagated_labels(patches, labels, \"wsi_labels_expe5.png\")\n",
    "\n",
    "# visualize_tsne_on_embeddings(sampled_embeddings,sampled_labels, \"kpca_labels_plot_expe5.png\")\n",
    "\n",
    "# intra_distances, inter_distances = calculate_approx_distances_in_embeddings(patches, labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e218d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import dash\n",
    "from dash import dcc, html\n",
    "from dash.dependencies import Input, Output\n",
    "import openslide\n",
    "from PIL import Image\n",
    "import io\n",
    "import base64\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "json_path = \"changed_labels.json\"\n",
    "changed_labels = load_dict(json_path)\n",
    "\n",
    "def get_patch_image(slide_path, x, y, patch_size=224):\n",
    "    slide = openslide.OpenSlide(slide_path)\n",
    "    tile = slide.read_region((x, y), 0, (patch_size, patch_size)).convert(\"RGB\")\n",
    "\n",
    "    buffered = io.BytesIO()\n",
    "    tile.save(buffered, format=\"PNG\")\n",
    "    return base64.b64encode(buffered.getvalue()).decode()\n",
    "\n",
    "def create_probability_bar_plot(probabilities):\n",
    "    fig, ax = plt.subplots(figsize=(5, 3))\n",
    "    ax.bar(np.arange(9), probabilities)\n",
    "    ax.set_xticks(np.arange(9))\n",
    "    ax.set_xlabel('Classes')\n",
    "    ax.set_ylabel('Probabilities')\n",
    "    ax.set_title('Distribution of probabilities')\n",
    "\n",
    "    buf = BytesIO()\n",
    "    plt.savefig(buf, format=\"png\")\n",
    "    buf.seek(0)\n",
    "    return base64.b64encode(buf.getvalue()).decode()\n",
    "\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"Modified labels visualization\"),\n",
    "    dcc.Dropdown(\n",
    "        id=\"category-filter\",\n",
    "        options=[{\"label\": f\"Class {i}\", \"value\": i} for i in range(9)],\n",
    "        value=0,\n",
    "        clearable=False\n",
    "    ),\n",
    "\n",
    "    html.Div(id=\"image-container\")\n",
    "])\n",
    "\n",
    "@app.callback(\n",
    "    Output(\"image-container\", \"children\"),\n",
    "    [Input(\"category-filter\", \"value\")]\n",
    ")\n",
    "def update_images(selected_category):\n",
    "    slide_path = \"C:/Users/lucas/AAA_MEMOIRE/Code_Memoire/img/database/13C08641.ndpi\"\n",
    "    filtered_labels = [entry for entry in changed_labels if entry[\"label\"] == selected_category]\n",
    "\n",
    "    images = []\n",
    "    for entry in filtered_labels[:20]:\n",
    "        x, y, pred, label = entry[\"x\"], entry[\"y\"], entry[\"pred\"], entry[\"label\"]\n",
    "        probabilities = np.random.rand(9)\n",
    "        img_base64 = get_patch_image(slide_path, x, y)\n",
    "        prob_plot_base64 = create_probability_bar_plot(probabilities)\n",
    "\n",
    "        images.append(html.Div([\n",
    "            html.Img(src=f\"data:image/png;base64,{img_base64}\", style={\"width\": \"150px\"}),\n",
    "            html.P(f\"Prédiction Initiale: {classes[pred]} → Nouvelle: {classes[label]}\"),\n",
    "            dcc.Graph(\n",
    "                id=f\"prob-bar-{x}-{y}\",\n",
    "                figure={\n",
    "                    \"data\": [\n",
    "                        {\"x\": np.arange(9), \"y\": probabilities, \"type\": \"bar\"}\n",
    "                    ],\n",
    "                    \"layout\": {\n",
    "                        \"title\": f\"\",\n",
    "                        \"xaxis\": {\"title\": \"Classes\"},\n",
    "                        \"yaxis\": {\"title\": \"Probabilité\"}\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "        ], style={\"display\": \"inline-block\", \"margin\": \"10px\"}))\n",
    "    \n",
    "    return images\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967a74e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from openslide import OpenSlide\n",
    "from PIL import Image, ImageDraw\n",
    "from reportlab.pdfgen import canvas\n",
    "\n",
    "slide_path = \"C:/Users/lucas/AAA_MEMOIRE/Code_Memoire/img/database/13C08641.ndpi\"\n",
    "ndpa_path = \"C:/Users/lucas/AAA_MEMOIRE/Code_Memoire/img/database/13C08641.ndpi.ndpa\"\n",
    "output_pdf = \"output2.pdf\"\n",
    "\n",
    "\n",
    "slide = OpenSlide(slide_path)\n",
    "\n",
    "if 7 >= len(slide.level_dimensions):\n",
    "    raise ValueError(\"La résolution 7 n'est pas disponible pour cette image.\")\n",
    "\n",
    "image = slide.read_region((0, 0), 6, slide.level_dimensions[6]).convert(\"RGB\")\n",
    "\n",
    "def parse_ndpa(ndpa_path):\n",
    "    \"\"\"Parses the NDPA file to extract annotations.\"\"\"\n",
    "    tree = ET.parse(ndpa_path)\n",
    "    root = tree.getroot()\n",
    "    annotations = []\n",
    "    for annotation in root.findall(\"Annotation\"):\n",
    "        coords = []\n",
    "        for point in annotation.findall(\"Coordinates/Coordinate\"):\n",
    "            x = int(float(point.get(\"X\")))\n",
    "            y = int(float(point.get(\"Y\")))\n",
    "            coords.append((x, y))\n",
    "        if coords:\n",
    "            annotations.append(coords)\n",
    "    return annotations\n",
    "\n",
    "annotations = parse_ndpa(ndpa_path)\n",
    "\n",
    "def draw_annotations(image, annotations):\n",
    "    \"\"\"Draws the extracted annotations onto the image.\"\"\"\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    for poly in annotations:\n",
    "        draw.polygon(poly, outline=\"red\", width=3)\n",
    "    return image\n",
    "\n",
    "image = draw_annotations(image, annotations)\n",
    "\n",
    "image.save(output_pdf, \"PDF\", resolution=100.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cc1761-de2d-4491-96c5-e2859e7adac5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import dash\n",
    "import dash_bootstrap_components as dbc\n",
    "from dash import dcc, html, Input, Output\n",
    "import plotly.express as px\n",
    "import cv2\n",
    "import numpy as np\n",
    "import base64\n",
    "import io\n",
    "from PIL import Image\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def parse_contents(contents):\n",
    "    content_type, content_string = contents.split(',')\n",
    "    decoded = base64.b64decode(content_string)\n",
    "    img = Image.open(io.BytesIO(decoded))\n",
    "    return np.array(img)\n",
    "\n",
    "def histogram_equalization(img, apply_clahe):\n",
    "    if not apply_clahe:\n",
    "        return img\n",
    "\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    img_clahe = clahe.apply(img_gray)\n",
    "    \n",
    "    img_clahe_rgb = cv2.cvtColor(img_clahe, cv2.COLOR_GRAY2RGB)\n",
    "    return img_clahe_rgb\n",
    "\n",
    "def kmeans_clustering(img, k):\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    img_reshaped = img_gray.reshape((-1, 1))\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(img_reshaped)\n",
    "    labels = kmeans.labels_\n",
    "    sorted_labels = np.argsort(kmeans.cluster_centers_.flatten())\n",
    "    sorted_img = np.zeros_like(labels)\n",
    "    for i, lbl in enumerate(sorted_labels):\n",
    "        sorted_img[labels == lbl] = i\n",
    "    return sorted_img.reshape(img_gray.shape)\n",
    "\n",
    "\n",
    "def morphological_operation(mask, operation, kernel_size):\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size, kernel_size))\n",
    "\n",
    "    if operation == 'dilate':\n",
    "        return cv2.dilate(mask, kernel, iterations=1)\n",
    "    elif operation == 'erode':\n",
    "        return cv2.erode(mask, kernel, iterations=1)\n",
    "    elif operation == 'open':\n",
    "        return cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "    elif operation == 'close':\n",
    "        return cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "    return mask\n",
    "\n",
    "def mask_original_image(img, clustered_img, selected_classes):\n",
    "    mask = np.isin(clustered_img, selected_classes).astype(np.uint8) * 255\n",
    "    masked_img = cv2.bitwise_and(img, img, mask=mask)\n",
    "    return masked_img\n",
    "\n",
    "app = dash.Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n",
    "\n",
    "app.layout = dbc.Container([\n",
    "    html.H1(\"Segmentation d'Image et Opérations Morphologiques\"),\n",
    "    dbc.Row([\n",
    "        dbc.Col(dcc.Upload(id='upload-image', children=html.Button('Charger une Image'), multiple=False), width=3),\n",
    "        dbc.Col(dcc.Checklist(id='apply-hist-eq', options=[{'label': 'Appliquer Equalization Histogram', 'value': 'yes'}], value=[]), width=3),\n",
    "        dbc.Col(dcc.Slider(2, 10, step=1, id='kmeans-k', value=5, marks={i: str(i) for i in range(2, 11)}), width=6)\n",
    "    ]),\n",
    "    dbc.Row([\n",
    "        dbc.Col(dcc.Graph(id='original-image'), width=4),\n",
    "        dbc.Col(dcc.Graph(id='equalized-image'), width=4),\n",
    "        dbc.Col(dcc.Graph(id='clustered-image'), width=4)\n",
    "    ]),\n",
    "    dbc.Row([\n",
    "        dbc.Col(dcc.Checklist(id='select-classes', options=[{'label': f'Classe {i}', 'value': i} for i in range(10)], value=[]), width=4),\n",
    "        dbc.Col(dcc.Dropdown(id='morph-operation', options=[\n",
    "            {'label': 'Dilate', 'value': 'dilate'},\n",
    "            {'label': 'Erode', 'value': 'erode'},\n",
    "            {'label': 'Open', 'value': 'open'},\n",
    "            {'label': 'Close', 'value': 'close'}\n",
    "        ], placeholder='Choisir une opération'), width=4),\n",
    "        dbc.Col(dcc.Slider(3, 15, step=2, id='kernel-size', value=5, marks={i: str(i) for i in range(3, 16, 2)}), width=4),\n",
    "        dbc.Col(dcc.Checklist(id='morph-classes', options=[{'label': f'Classe {i}', 'value': i} for i in range(10)], value=[]), width=4)\n",
    "\n",
    "    ]),\n",
    "    dbc.Row([\n",
    "        dbc.Col(dcc.Graph(id='filtered-image'), width=4),\n",
    "        dbc.Col(dcc.Graph(id='morph-image'), width=4),\n",
    "        dbc.Col(dcc.Graph(id='morph-filtered-image'), width=4)\n",
    "    ])\n",
    "])\n",
    "\n",
    "@app.callback(\n",
    "    [Output('original-image', 'figure'),\n",
    "     Output('equalized-image', 'figure'),\n",
    "     Output('clustered-image', 'figure'),\n",
    "     Output('filtered-image', 'figure'),\n",
    "     Output('morph-image', 'figure'),\n",
    "     Output('morph-filtered-image', 'figure')],\n",
    "    [Input('upload-image', 'contents'),\n",
    "     Input('apply-hist-eq', 'value'),\n",
    "     Input('kmeans-k', 'value'),\n",
    "     Input('morph-operation', 'value'),\n",
    "     Input('kernel-size', 'value'),\n",
    "     Input('morph-classes', 'value'),\n",
    "     Input('select-classes', 'value')]\n",
    ")\n",
    "def update_image(contents, hist_eq, k, morph_operation, kernel_size, morph_classes, selected_classes):\n",
    "    if contents is None:\n",
    "        return [px.imshow(np.zeros((100, 100, 3), dtype=np.uint8))] * 6\n",
    "    \n",
    "    img = parse_contents(contents)\n",
    "    img_eq = histogram_equalization(img, 'yes' in hist_eq)\n",
    "    clustered_img = kmeans_clustering(img_eq, k)\n",
    "    \n",
    "    masked_img = mask_original_image(img, clustered_img, selected_classes)\n",
    "    morph_img = clustered_img.copy()\n",
    "\n",
    "    if morph_operation:\n",
    "        for morph_class in morph_classes:\n",
    "            class_mask = (clustered_img == morph_class).astype(np.uint8) * 255\n",
    "            processed_mask = morphological_operation(class_mask, morph_operation, kernel_size)\n",
    "            morph_img = np.where(processed_mask == 255, morph_class, morph_img)\n",
    "    \n",
    "    morph_filtered_img = mask_original_image(img, morph_img, selected_classes)\n",
    "\n",
    "    return [px.imshow(img), px.imshow(img_eq), px.imshow(clustered_img), px.imshow(masked_img), px.imshow(morph_img), px.imshow(morph_filtered_img)]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e3cfd3-89d3-4d5c-b227-69d5c4d66649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dash\n",
    "import dash_bootstrap_components as dbc\n",
    "from dash import dcc, html, Input, Output\n",
    "import plotly.express as px\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "img_path = \"cells_morph.png\"\n",
    "img = cv2.imread(img_path)\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def morphological_operation(img, operation, kernel_size):\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size, kernel_size))\n",
    "\n",
    "    if operation == 'dilate':\n",
    "        return cv2.dilate(img, kernel, iterations=1)\n",
    "    elif operation == 'erode':\n",
    "        return cv2.erode(img, kernel, iterations=1)\n",
    "    elif operation == 'open':\n",
    "        return cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)\n",
    "    elif operation == 'close':\n",
    "        return cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)\n",
    "    return img\n",
    "\n",
    "def watershed_segmentation(img):\n",
    "    \n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    \n",
    "    # Noise removal - dilate et erode\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    sure_bg = cv2.dilate(thresh, kernel, iterations=3)\n",
    "  \n",
    "    sure_fg = cv2.erode(thresh, kernel, iterations=3)\n",
    "    \n",
    "    dist_transform = cv2.distanceTransform(thresh, cv2.DIST_L2, 5)\n",
    "    _, sure_fg = cv2.threshold(dist_transform, 0.7 * dist_transform.max(), 255, 0)\n",
    "    \n",
    "    sure_fg = np.uint8(sure_fg)\n",
    "    unknown = cv2.subtract(sure_bg, sure_fg)\n",
    "    \n",
    "    _, markers = cv2.connectedComponents(sure_fg)\n",
    "    markers = markers + 1\n",
    "    markers[unknown == 255] = 0\n",
    "    cv2.watershed(img, markers)\n",
    "    \n",
    "    img[markers == -1] = [255, 0, 0]\n",
    "    \n",
    "    contours, _ = cv2.findContours(markers.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    areas = [cv2.contourArea(c) for c in contours]\n",
    "    \n",
    "    areas.sort(reverse=True)\n",
    "    max_area = areas[1] if len(areas) > 1 else 0\n",
    "    \n",
    "    return img, max_area\n",
    "\n",
    "app = dash.Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n",
    "\n",
    "app.layout = dbc.Container([\n",
    "    html.H1(\"Opérations Morphologiques et Watershed sur Image\"),\n",
    "    dbc.Row([\n",
    "        dbc.Col(dcc.Dropdown(\n",
    "            id='morph-operation',\n",
    "            options=[\n",
    "                {'label': 'Erode', 'value': 'erode'},\n",
    "                {'label': 'Dilate', 'value': 'dilate'},\n",
    "                {'label': 'Close', 'value': 'close'},\n",
    "                {'label': 'Open', 'value': 'open'}\n",
    "            ],\n",
    "            value='dilate',\n",
    "            placeholder=\"Choisir une opération morphologique\"\n",
    "        ), width=4),\n",
    "        dbc.Col(dcc.Slider(\n",
    "            id='kernel-size',\n",
    "            min=3,\n",
    "            max=15,\n",
    "            step=2,\n",
    "            value=5,\n",
    "            marks={i: str(i) for i in range(3, 16, 2)},\n",
    "            tooltip={\"placement\": \"bottom\", \"always_visible\": True}\n",
    "        ), width=8),\n",
    "    ]),\n",
    "    dbc.Row([\n",
    "        dbc.Col(dcc.Graph(id='original-image'), width=6),\n",
    "        dbc.Col(dcc.Graph(id='morph-image'), width=6),\n",
    "    ]),\n",
    "    dbc.Row([\n",
    "        dbc.Col(html.Div(id='largest-area'), width=12),\n",
    "    ])\n",
    "])\n",
    "\n",
    "@app.callback(\n",
    "    [Output('original-image', 'figure'),\n",
    "     Output('morph-image', 'figure'),\n",
    "     Output('largest-area', 'children')],\n",
    "    [Input('morph-operation', 'value'),\n",
    "     Input('kernel-size', 'value')]\n",
    ")\n",
    "def update_image(operation, kernel_size):\n",
    "    morph_img = morphological_operation(img_rgb, operation, kernel_size)\n",
    "   \n",
    "    watershed_img, max_area = watershed_segmentation(morph_img)\n",
    "\n",
    "    original_fig = px.imshow(img_rgb, title=\"Image Originale\")\n",
    "    morph_fig = px.imshow(watershed_img, title=f\"Image après {operation.capitalize()} et Watershed\")\n",
    "    \n",
    "    return original_fig, morph_fig, f\"Area of biggest object detected is : {max_area:.2f} pixels squared\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd6ce51-b793-4eee-9093-a3d2bb2162fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage.segmentation import watershed\n",
    "from skimage.feature import peak_local_max\n",
    "\n",
    "def histogram_equalization(img):\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    img_clahe = clahe.apply(img_gray)\n",
    "    \n",
    "    return img_clahe\n",
    "\n",
    "def kmeans_clustering(img_gray, n_clusters=4):\n",
    "    img_reshaped = img_gray.reshape((-1, 1))\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(img_reshaped)\n",
    "    labels = kmeans.labels_.reshape(img_gray.shape)\n",
    "    return labels, kmeans.cluster_centers_\n",
    "\n",
    "def morphological_operations(mask, kernel_size=9):\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size, kernel_size))\n",
    "    opened = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "    closed = cv2.morphologyEx(opened, cv2.MORPH_CLOSE, kernel)\n",
    "    return closed\n",
    "\n",
    "def watershed_segmentation(mask):\n",
    "    sure_bg = cv2.dilate(mask, None, iterations=1)\n",
    "    sure_fg = cv2.erode(mask, None, iterations=1)\n",
    "    unknown = cv2.subtract(sure_bg, sure_fg)\n",
    "\n",
    "    img_watershed = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n",
    "\n",
    "    markers = np.zeros_like(mask, dtype=np.int32)\n",
    "    markers[sure_fg == 255] = 1\n",
    "    markers[unknown == 255] = 0\n",
    "\n",
    "    cv2.watershed(img_watershed, markers)\n",
    "\n",
    "    img_watershed[markers == -1] = [255, 0, 0]\n",
    "\n",
    "    contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    areas = [cv2.contourArea(c) for c in contours]\n",
    "    max_area = max(areas) if areas else 0\n",
    "    return img_watershed, max_area\n",
    "\n",
    "\n",
    "img = cv2.imread(\"labels/41/41_1.png\")\n",
    "\n",
    "img_eq = histogram_equalization(img)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Image Originale\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.imshow(img_eq, cmap='gray')\n",
    "plt.title(\"Image après Histogram Equalization\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "labels, cluster_centers = kmeans_clustering(img_eq, n_clusters=4)\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.imshow(labels, cmap='viridis')\n",
    "plt.title(\"Labels après K-Means\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "sorted_labels = np.argsort(cluster_centers.flatten())\n",
    "sorted_img = np.zeros_like(labels)\n",
    "for i, lbl in enumerate(sorted_labels):\n",
    "    sorted_img[labels == lbl] = i\n",
    "\n",
    "mask = np.isin(sorted_img, [1, 2]).astype(np.uint8) * 255 \n",
    "plt.subplot(2, 3, 4)\n",
    "plt.imshow(mask, cmap='gray')\n",
    "plt.title(\"Masque des Catégories 2 et 3\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "morph_img = morphological_operations(mask)\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.imshow(morph_img, cmap='gray')\n",
    "plt.title(\"Image après Opening et Closing\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "watershed_img, max_area = watershed_segmentation(morph_img)\n",
    "print(img.shape[0], img.shape[1])\n",
    "\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.imshow(cv2.cvtColor(watershed_img, cv2.COLOR_BGR2RGB))\n",
    "plt.title(f\"Image après Watershed (Aire max : {(100*max_area)/(img.shape[0]*img.shape[1])} %)\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6882dd-36a5-40c7-9cc9-6bd85db93f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from skimage.segmentation import watershed\n",
    "import large_image\n",
    "\n",
    "def histogram_equalization(img):\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    img_clahe = clahe.apply(img_gray)\n",
    "    \n",
    "    return img_clahe\n",
    "\n",
    "def kmeans_clustering(img_gray, n_clusters=4):\n",
    "    img_reshaped = img_gray.reshape((-1, 1))\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(img_reshaped)\n",
    "    labels = kmeans.labels_.reshape(img_gray.shape)\n",
    "    return labels, kmeans.cluster_centers_\n",
    "\n",
    "def morphological_operations(mask, kernel_size=9):\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size, kernel_size))\n",
    "    opened = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "    closed = cv2.morphologyEx(opened, cv2.MORPH_CLOSE, kernel)\n",
    "    return closed\n",
    "\n",
    "def watershed_segmentation(mask):\n",
    "    sure_bg = cv2.dilate(mask, None, iterations=3)\n",
    "    sure_fg = cv2.erode(mask, None, iterations=3)\n",
    "    unknown = cv2.subtract(sure_bg, sure_fg)\n",
    "\n",
    "    img_watershed = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n",
    "\n",
    "    markers = np.zeros_like(mask, dtype=np.int32)\n",
    "    markers[sure_fg == 255] = 1\n",
    "    markers[unknown == 255] = 0\n",
    "\n",
    "    cv2.watershed(img_watershed, markers)\n",
    "    img_watershed[markers == -1] = [255, 0, 0]\n",
    "\n",
    "    contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    areas = [cv2.contourArea(c) for c in contours]\n",
    "    max_area = max(areas) if areas else 0\n",
    "    return img_watershed, max_area\n",
    "\n",
    "def get_area_directory(area_percentage):\n",
    "    if area_percentage < 1:\n",
    "        return \"output_tiles/less_than_1_percent\"\n",
    "    elif 1 <= area_percentage < 3:\n",
    "        return \"output_tiles/1_to_3_percent\"\n",
    "    elif 3 <= area_percentage < 5:\n",
    "        return \"output_tiles/3_to_5_percent\"\n",
    "    else:\n",
    "        return \"output_tiles/greater_than_5_percent\"\n",
    "\n",
    "def process_tiles(image_path, magnification, patch_w, patch_h):\n",
    "    image_slide = large_image.getTileSource(image_path)\n",
    "    \n",
    "    os.makedirs(\"output_tiles/less_than_1_percent\", exist_ok=True)\n",
    "    os.makedirs(\"output_tiles/1_to_3_percent\", exist_ok=True)\n",
    "    os.makedirs(\"output_tiles/3_to_5_percent\", exist_ok=True)\n",
    "    os.makedirs(\"output_tiles/greater_than_5_percent\", exist_ok=True)\n",
    "\n",
    "    for slide_info in image_slide.tileIterator(\n",
    "            scale=image_slide.getMagnificationForLevel(magnification),\n",
    "            tile_size=dict(width=patch_w, height=patch_h),\n",
    "            tile_overlap=dict(x=0, y=0),\n",
    "            format=large_image.tilesource.TILE_FORMAT_NUMPY):\n",
    "        \n",
    "        x, y = slide_info['x'], slide_info['y']\n",
    "        if x < 25000 or y < 25000:\n",
    "            continue\n",
    "        print(f\"Processing tile at ({x}, {y})\")\n",
    "        tile = np.array(slide_info['tile'])\n",
    "\n",
    "        img_eq = histogram_equalization(tile)\n",
    "\n",
    "        labels, cluster_centers = kmeans_clustering(img_eq, n_clusters=4)\n",
    "\n",
    "        sorted_labels = np.argsort(cluster_centers.flatten())\n",
    "        sorted_img = np.zeros_like(labels)\n",
    "        for i, lbl in enumerate(sorted_labels):\n",
    "            sorted_img[labels == lbl] = i\n",
    "\n",
    "        mask = np.isin(sorted_img, [1, 2]).astype(np.uint8) * 255\n",
    "\n",
    "        morph_img = morphological_operations(mask)\n",
    "        watershed_img, max_area = watershed_segmentation(morph_img)\n",
    "        total_area = tile.shape[0] * tile.shape[1]\n",
    "        area_percentage = (100 * max_area) / total_area\n",
    "\n",
    "        output_dir = get_area_directory(area_percentage)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        tile_rgb = cv2.cvtColor(tile, cv2.COLOR_BGR2RGB)\n",
    "        output_filename = f\"{output_dir}/{int(area_percentage)}_{x}_{y}.png\"\n",
    "        cv2.imwrite(output_filename, tile_rgb)\n",
    "\n",
    "        print(f\"Saved tile at ({x}, {y}) with area percentage: {area_percentage:.2f}%\")\n",
    "\n",
    "process_tiles(\n",
    "    image_path=\"large_image_source_tifffile/DataBase/22C06901.ndpi\",\n",
    "    magnification=10.0,\n",
    "    patch_w=1000,\n",
    "    patch_h=1000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3886e9c6-6da9-44c3-8b3e-f6b4589945b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def plot_rgb_histogram(img):\n",
    "    \"\"\"Display the histogram for Red, Green, and Blue channels.\"\"\"\n",
    "    colors = ('b', 'g', 'r')\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    for i, color in enumerate(colors):\n",
    "        hist = cv2.calcHist([img], [i], None, [256], [0, 256])\n",
    "        plt.plot(hist, color=color, label=f'{color.upper()} Channel')\n",
    "    \n",
    "    plt.title(\"RGB Channel Histogram\")\n",
    "    plt.xlabel(\"Pixel Intensity\")\n",
    "    plt.ylabel(\"Number of Pixels\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def kmeans_color_quantization(img, k=5):\n",
    "    \"\"\"Apply K-Means to detect dominant colors.\"\"\"\n",
    "    img_reshaped = img.reshape((-1, 3))  # Convert to 2D array\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(img_reshaped)\n",
    "    \n",
    "    # Get dominant colors\n",
    "    dominant_colors = kmeans.cluster_centers_.astype(np.uint8)\n",
    "    \n",
    "    return dominant_colors\n",
    "\n",
    "def watershed_segmentation(image_path):\n",
    "    \"\"\"Perform Watershed segmentation with preprocessing and K-Means.\"\"\"\n",
    "    \n",
    "    #Load the image\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    #Display histogram\n",
    "    plot_rgb_histogram(img)\n",
    "\n",
    "    #Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    #Apply histogram equalization to improve contrast\n",
    "    gray_eq = cv2.equalizeHist(gray)\n",
    "\n",
    "    #Apply Gaussian blur to reduce noise\n",
    "    blurred = cv2.GaussianBlur(gray_eq, (5, 5), 0)\n",
    "\n",
    "    #Otsu thresholding\n",
    "    _, binary = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    #Distance transform for Watershed\n",
    "    dist_transform = cv2.distanceTransform(binary, cv2.DIST_L2, 5)\n",
    "    _, sure_fg = cv2.threshold(dist_transform, 0.5 * dist_transform.max(), 255, 0)\n",
    "\n",
    "    sure_fg = np.uint8(sure_fg)\n",
    "    unknown = cv2.subtract(binary, sure_fg)\n",
    "\n",
    "    #Connected component labeling\n",
    "    markers = cv2.connectedComponents(sure_fg)[1]\n",
    "    markers += 1\n",
    "    markers[unknown == 255] = 0\n",
    "\n",
    "    #Apply Watershed\n",
    "    cv2.watershed(img, markers)\n",
    "    img[markers == -1] = [255, 0, 0]  # Red boundaries\n",
    "\n",
    "    #Detect dominant colors with K-Means\n",
    "    k = 4\n",
    "    dominant_colors = kmeans_color_quantization(img, k=k)\n",
    "\n",
    "    #Display results\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    axes[0].imshow(gray_eq, cmap=\"gray\")\n",
    "    axes[0].set_title(\"Equalized Grayscale Image\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    axes[1].imshow(img)\n",
    "    axes[1].set_title(\"Watershed Boundaries\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    axes[2].bar(range(k), [1]*k, color=[dominant_colors[i] / 255 for i in range(k)])\n",
    "    axes[2].set_title(\"Dominant Colors\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "#Run segmentation on test image\n",
    "watershed_segmentation(\"test.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "darts_env",
   "language": "python",
   "name": "darts_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
